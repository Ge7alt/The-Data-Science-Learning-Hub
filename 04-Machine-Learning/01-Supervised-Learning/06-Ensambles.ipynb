{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensables\n",
    "\n",
    "Ensemble methods combine multiple machine learning models to improve overall predictive performance. The idea is that by aggregating the predictions of multiple models, the ensemble can achieve better accuracy and robustness compared to any individual model.\n",
    "\n",
    "There are four most commonly used methods of ensambles:\n",
    "\n",
    "- **Bagging:** Bagging, short for bootstrap aggregating, is a technique that involves training multiple models on different subsets of the training data, and then combining their predictions by averaging (for regression) or majority voting (for classification). The subsets are created by sampling the training data with replacement, which means that some instances may be present in multiple subsets while others may be omitted.\n",
    "\n",
    "- **Boosting:** Boosting is another ensemble technique that focuses on sequentially training models, where each subsequent model learns from the mistakes made by the previous ones. The models are trained iteratively, and each new model gives more weight to the instances that were misclassified by the previous models.\n",
    "\n",
    "- **Stacking:** Stacking is a technique that combines the predictions of multiple models by training a meta-model on their outputs. It involves creating a new dataset where the features are the predictions made by the base models, and the target variable is the true target from the training data. The meta-model is then trained on this dataset to make the final predictions.\n",
    "\n",
    "- **Cascading:** Cascading is a technique that combines multiple models in a sequence, where the output of one model is used as an input for the next model in the cascade. Each model in the cascade focuses on a specific aspect of the problem, allowing for a more refined and accurate prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random Forest is the most popular bagging model  that combines the power of decision trees and ensemble learning. It is known for its robustness, accuracy, and ability to handle complex problems.\n",
    "\n",
    "### Building Decision Trees\n",
    "\n",
    "Random Forests consist of an ensemble of decision trees ($M_1$, $M_2$, ..., $M_k$). Each decision tree is built using a random subset of the training data ($D_1$, $D_2$, ..., $D_k$). The idea is to introduce randomness and diversity among the trees. Here $k$ represents the total number of data subsamples.\n",
    "\n",
    "![Random Forest](./../../assets/random-forest.jpg)\n",
    "\n",
    "To build a decision tree, the algorithm selects a random subset of features from the dataset. It then uses these features to split the data at each node based on certain criteria (e.g., Gini impurity or entropy). The splitting continues recursively until a stopping condition is met, such as reaching a maximum depth or having a minimum number of instances in each leaf node.\n",
    "\n",
    "### Bagging and Voting\n",
    "\n",
    "Random Forests employ a technique called \"bagging\" (bootstrap aggregating) to create multiple decision trees. Bagging involves sampling the training data with replacement to create different subsets for each tree. This means that some instances may appear multiple times in a subset, while others may be left out.\n",
    "\n",
    "Once the decision trees are built, they make predictions individually. Here each model ($M_1$, $M_2$, ..., $M_k$) has seen different sets of data ($D_1$, $D_2$, ..., $D_k$). The combination of all these models to form a new ultimate model $M$ is known as aggregation and the models ($M_1$, $M_2$, ..., $M_k$) are known as bootstrap samples. For classification tasks, each tree votes for the class label, and the class with the majority of votes is selected as the final prediction. For regression tasks, the trees' predictions are averaged to obtain the final prediction.\n",
    "\n",
    "If we change the $D_{Train}$, all the models won't change. Only a few models whose sample data $D_k$ changes will change. So, there is very low chance that the aggregate model $M$ changes. This reduces variance for bagging models without necessarily increasing the bias. This gives and insight that bagging works well for models that have low bias and high variance.\n",
    "\n",
    "$error = {bias}^2 + variance$\n",
    "\n",
    "### Bias-Variance Tradeoff\n",
    "\n",
    "Random Forests aim to strike a balance between bias and variance. Here's how the bias-variance tradeoff is managed in Random Forests:\n",
    "\n",
    "- **Bias:** Each decision tree in a Random Forest has inherent bias. Decision trees can be prone to high bias because they can only represent relatively simple relationships between features and the target variable. However, by aggregating multiple decision trees, the Random Forest can reduce the overall bias. The ensemble approach allows the Random Forest to capture more complex relationships and improve the model's ability to make accurate predictions.\n",
    "\n",
    "- **Variance:** The Random Forest's strength lies in its ability to reduce variance. Since each decision tree is trained on a different subset of the training data, and feature subsets are considered at each split, the individual decision trees in the Random Forest exhibit different sources of variance. When combined, the Random Forest averages out the individual tree's predictions, reducing the overall variance and making the model more robust to noise and outliers.\n",
    "\n",
    "By combining multiple decision trees and aggregating their predictions, Random Forests can effectively manage the bias-variance tradeoff. They reduce bias by capturing more complex relationships through ensemble learning and reduce variance by averaging the predictions of multiple trees.\n",
    "\n",
    "It's worth noting that the bias-variance tradeoff in Random Forests can be influenced by the following factors:\n",
    "\n",
    "- **Number of Trees (k):** Increasing the number of trees in the Random Forest can help reduce variance further. However, there is a point of diminishing returns where adding more trees may not yield significant improvements and may lead to increased computational costs.\n",
    "\n",
    "- **Tree Depth:** Controlling the depth of each decision tree can also affect the bias-variance tradeoff. Shallow trees may have high bias but low variance, whereas deeper trees can capture more complex patterns but may suffer from higher variance. Properly tuning the tree depth can help strike a balance between bias and variance.\n",
    "\n",
    "- **Feature Subsampling:** Random Forests randomly select a subset of features at each split. This technique, known as feature subsampling or feature bagging, helps reduce the correlation between trees and improves the model's generalization by reducing variance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
