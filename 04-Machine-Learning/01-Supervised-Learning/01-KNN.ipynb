{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (k-NN)\n",
    "\n",
    "The k-nearest neighbors algorithm is a supervised machine learning algorithm used for classification and regression tasks. It is a non-parametric method that makes predictions based on the similarity of the input data to its neighbors. The algorithm assumes that similar data points tend to have similar labels.\n",
    "\n",
    "The `k` in k-NN refers to the number of nearest neighbors that will be considered when making predictions. The algorithm calculates the distance between the input data and all the training data points, selects the `k` nearest neighbors, and determines the class or value based on the majority vote (for classification) or averaging (for regression) of the neighbors.\n",
    "\n",
    "Before implementing the k-NN algorithm, you need to decide the value of `k`, i.e., the number of nearest neighbors to consider. The choice of `k` depends on the dataset and problem at hand. \n",
    "\n",
    "- **Small k (Overfitting):** When the value of k is small, the k-NN algorithm can suffer from overfitting. Overfitting occurs when the model becomes too complex and tries to fit the training data too closely, leading to poor generalization on unseen data. In the case of small k, the decision boundary can become highly irregular and sensitive to noise in the training data. The model can end up capturing the noise or specific patterns in the training set, which may not generalize well to new data.\n",
    "\n",
    "- **Large k (Underfitting):** Conversely, when the value of k is large, the k-NN algorithm can suffer from underfitting. Underfitting occurs when the model is too simple and fails to capture the underlying patterns in the data. In the case of large k, the decision boundary becomes smoother and less sensitive to noise. The model may oversimplify the data and fail to capture intricate patterns, resulting in high bias and low variance.\n",
    "\n",
    "![KNN Overfitting and Underfitting](./../../assets/knn.jpg)\n",
    "\n",
    "If `n` be the total number of data points then from the figure above we can draw the insight that:\n",
    "- When `k=1`, decision surface becomes more zig-zagged i.e. the system is doing more work to remove noisy outliers or to fit to the system. The process is known as overfitting. This works perfectly for the training set, because we know both the labels and features of our data for training set. But when it comes to unseen data, overfitted model fails to conserve the performance.\n",
    "- when `k=n` the decision surface is smooth i.e. the system is not doing any work to remove the noisy outliers. This process is known as underfitting. Let's say we have `n=1000` where 600 data points belong to positive group and remaining 400 to negative group. When `k=n` whatever the type of given point be, it takes 600 positive neighbors and 400 negative neighbors. So the final result will be positive no matter what class it represents. This is underfitting.\n",
    "\n",
    "So in order to remove overfitting and underfitting, we need to choose the right value of `k` for our problem. This can be achieved using cross-validation set of data.\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "Cross-validation is a resampling technique used to assess the performance and generalization ability of a machine learning model. It helps in estimating how well the model will perform on unseen data by simulating the model's performance on multiple subsets of the available data.\n",
    "\n",
    "The basic idea behind cross-validation is to split the dataset into multiple subsets called \"folds.\" The model is trained on a subset of the data called the \"training set\" and evaluated on the remaining portion of the data called the \"validation set\". The validation set is used as a test-set during the training period. This process is repeated several times, with different subsets of the data serving as the validation set each time. During these iterations we choose the value of `k` that minimizes the loss for validation set.\n",
    "\n",
    "### k-fold cross-validation\n",
    "\n",
    "In k-fold cross-validation `k` represents the number of folds. Let's see step by step overview of k-fold cross-validation.\n",
    "\n",
    "- Start by splitting your dataset into two parts: a training set and a test set. The training set will be used for model training and validation, while the test set will be kept aside for final evaluation. Typically, a common split is 70-80% of the data for training and 20-30% for testing.\n",
    "- Determine the value of `k`, which represents the number of folds or subsets to divide the training data into. A common choice is `k = 5` or `k = 10`, but you can experiment with different values depending on the size of your dataset.\n",
    "- Divide the training set into `k` equally sized folds or subsets. Each fold should ideally have a similar distribution of samples. For example, if you have 100 samples and choose `k = 5`, each fold would contain 20 samples.\n",
    "- Perform the following steps `k` times:\n",
    "    - Choose one fold as the validation set and keep it aside. This fold will be used for evaluating the model.\n",
    "    - Train your model using the remaining `k-1` folds (the training set).\n",
    "    - Use the trained model to make predictions on the validation set (the fold that was kept aside). Calculate the performance metrics of interest, such as accuracy, precision, recall, or others.\n",
    "    - Repeat steps `a` to `c` for each fold, ensuring that each fold serves as the validation set exactly once.\n",
    "- Once the k-fold cross-validation process is complete, calculate the average performance metric across all the folds. This provides an overall estimate of the model's performance.\n",
    "- Analyze the performance metrics obtained from the cross-validation process to select the best-performing model or tune the model's hyperparameters. You can compare the performance of different models or different sets of hyperparameters across the folds to make an informed decision.\n",
    "- After selecting the best model or hyperparameters using cross-validation, evaluate the final model on the separate test set that was initially set aside. This provides an unbiased estimate of the model's performance on completely unseen data.\n",
    "\n",
    "Remember: k-fold cross-validation maximizes the use of available data, provides a more reliable estimate of the model's performance, and helps in selecting the best model or hyperparameters.\n",
    "\n",
    "## Determining Overfitting and Underfitting\n",
    "\n",
    "Before understanding overfitting and underfitting we need to have some primal informations. The error on the training set is known as training error. Similarly validation error and test error are the errors on validation set and test set respectively. Also error is calculates as:\n",
    "\n",
    "$error = 1 - accuracy$\n",
    "\n",
    "![Determining Overfitting and Underfitting](./../../assets/fitting.jpg)\n",
    "\n",
    "From the figure above we can draw the insight that:\n",
    "\n",
    "- If both validation error and training error are high, we are overfitting\n",
    "- If validation error is high but training error is low, we are underfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing KNN\n",
    "\n",
    "Now, let's dive into the step-by-step process of implementing the k-NN algorithm.\n",
    "\n",
    "### Step 1: Collect and Preprocess the Data\n",
    "The first step is to collect and preprocess the data. Ensure that your data is properly labeled for classification tasks or contains corresponding output values for regression tasks. Also, make sure to handle any missing or inconsistent data.\n",
    "\n",
    "### Step 2: Determine the Value of k\n",
    "Before implementing the k-NN algorithm, you need to decide the value of k, i.e., the number of nearest neighbors to consider. The choice of k depends on the dataset and problem at hand. A small value of k may lead to overfitting, while a large value may result in underfitting. Experimentation and cross-validation techniques can help determine the optimal value for k.\n",
    "\n",
    "### Step 3: Define a Distance Metric\n",
    "To calculate the similarity or distance between data points, you need to define a distance metric. The most commonly used distance metric is Euclidean distance. For two points (x1, y1) and (x2, y2), the Euclidean distance is calculated as:\n",
    "\n",
    "```python\n",
    "distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    "```\n",
    "\n",
    "However, depending on the type of data and problem, other distance metrics like Manhattan distance, Minkowski distance, or cosine similarity can also be used.\n",
    "\n",
    "### Step 4: Implement the k-NN Algorithm\n",
    "\n",
    "Now, let's implement the k-NN algorithm. We'll cover both the classification and regression versions.\n",
    "\n",
    "- **Classification**\n",
    "    - Load the training dataset.\n",
    "    - Load the test data for which you want to predict the labels.\n",
    "    - For each test data point:\n",
    "        - Calculate the distance between the test point and all the training data points using the chosen distance metric.\n",
    "        - Sort the distances in ascending order.\n",
    "        - Select the top k nearest neighbors.\n",
    "        - Determine the majority class of the k neighbors.\n",
    "        - Assign the majority class as the predicted label for the test point.\n",
    "    - Evaluate the accuracy of the predictions by comparing them with the true labels.\n",
    "\n",
    "- **Regression**\n",
    "    - Load the training dataset.\n",
    "    - Load the test data for which you want to predict the values.\n",
    "    - For each test data point:\n",
    "        - Calculate the distance between the test point and all the training data points using the chosen distance metric.\n",
    "        - Sort the distances in ascending order.\n",
    "        - Select the top k nearest neighbors.\n",
    "        - Determine the average value of the k neighbors.\n",
    "        - Assign the average value as the predicted value for the test point.\n",
    "    - Evaluate the performance of the predictions using appropriate metrics like mean squared error (MSE) or R-squared.\n",
    "\n",
    "### Step 5: Evaluate and Tune the Model\n",
    "Once you have implemented the k-NN algorithm, it's crucial to evaluate the model's performance. For classification tasks, you can use metrics like accuracy, precision, recall, or F1-score. For regression tasks, metrics like MSE, R-squared, or mean absolute error (MAE) can be used.\n",
    "\n",
    "You can also fine-tune the model by experimenting with different values of k, trying different distance metrics, or applying feature scaling techniques to normalize the data.\n",
    "\n",
    "`Example:` k-NN for Classification\n",
    "\n",
    "Let's walk through an example of using k-NN for a classification task. We'll consider a dataset with two features (x1 and x2) and two classes (A and B).\n",
    "\n",
    "1. Collect and preprocess the data.\n",
    "2. Determine the value of k, let's say k = 3.\n",
    "3. Define the distance metric, which will be the Euclidean distance.\n",
    "4. Implement the k-NN algorithm.\n",
    "\n",
    "Suppose we have the following training dataset:\n",
    "\n",
    "| Data Point | x1 | x2 | Class |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| 1\t| 1.0 | 1.1 |A |\n",
    "| 2\t| 1.0 | 1.0 |A |\n",
    "| 3\t| 0.0 | 0.0 |B |\n",
    "| 4\t| 0.0 | 0.1 |B |\n",
    "\n",
    "And the test dataset:\n",
    "\n",
    "|Test Point\t| x1 | x2 |\n",
    "|:-:|:-:|:-:|\n",
    "|5 | 0.9 | 0.8 |\n",
    "\n",
    "Let's calculate the distances between the test point (5) and the training points:\n",
    "\n",
    "- $Distance(5,1) = \\sqrt{(1.0 - 0.9)^2 + (1.1 - 0.8)^2} = 0.3$\n",
    "- $Distance(5,2) = \\sqrt{(1.0 - 0.9)^2 + (1.0 - 0.8)^2} = 0.223$\n",
    "- $Distance(5,3) = \\sqrt{(0.0 - 0.9)^2 + (0.0 - 0.8)^2} = 1.13$\n",
    "- $Distance(5,4) = \\sqrt{(0.0 - 0.9)^2 + (0.1 - 0.8)^2} = 1.18$\n",
    "\n",
    "Sorting the distances in ascending order, we get: 0.223, 0.3, 1.13, 1.18\n",
    "\n",
    "The top `k` neighbors are: (2, 1), (1, 1), (4, B)\n",
    "\n",
    "The majority class among the `k` neighbors is `A`, so the predicted label for the test point `(5)` is `A`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
