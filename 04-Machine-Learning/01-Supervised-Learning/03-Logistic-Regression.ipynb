{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is a popular statistical model used for binary classification problems. It aims to estimate the probability that an instance belongs to a particular class. The model assumes a linear relationship between the predictors and the log-odds of the event occurring. In this response, I'll explain the geometrical intuition behind logistic regression and provide the necessary formulas.\n",
    "\n",
    "- **Assumption:** Classes are almost perfectly linerarly separable.\n",
    "\n",
    "## Geometrical Intuition\n",
    "\n",
    "In logistic regression, we can interpret the output as the probability of an instance belonging to a particular class. The decision boundary that separates the two classes is a hyperplane in the feature space. Since logistic regression is a binary classifier, this hyperplane divides the feature space into two regions—one for each class.\n",
    "\n",
    "The decision boundary is determined by the weights (coefficients) assigned to the predictors. These weights control the orientation and tilt of the hyperplane. By adjusting the weights, logistic regression finds the best-fitting decision boundary that maximizes the likelihood of the observed data.\n",
    "\n",
    "In logistic regression, we take $y=+1$ for positive points and $y=-1$ for negative points. Also $y=mx$ in higher dimension becomes $w^Tx+b=0$, where $w$ is normal drawn from the plane to the current point $x$ and $b$ is the intercept.\n",
    "\n",
    "When plane passes through origin $b=0$ so the equatio becomes $w^Tx=0$\n",
    "\n",
    "![Logistic Regression](./../../assets/logistic.jpg)\n",
    "\n",
    "From Figure,\n",
    "$distance(S_i) = \\frac{w^Tx}{||w||}$\n",
    "\n",
    "If $||w||$ is a unit vector i.e. $||w||=1$\n",
    "\n",
    "$S_i = w^Tx_i > 0$\n",
    "\n",
    "$S_j = w^Tx_j < 0$\n",
    "\n",
    "$y_i * w^Tx_i > 0$ means $w$ is correctly classifying the point while $y_i * w^Tx_i < 0$ means $w$ is misclassifying the point\n",
    "\n",
    "- **Case I:** $y_i = +ve$ and $w^Tx_i = +ve$ then $y_i * w^Tx_i > 0$ i.e. $x_i$ is correctly classified\n",
    "- **Case II:** $y_i = -ve$ and $w^Tx_i = -ve$ then $y_i * w^Tx_i > 0$ i.e. $x_i$ is correctly classified\n",
    "- **Case III:** $y_i = +ve$ and $w^Tx_i = -ve$ then $y_i * w^Tx_i < 0$ i.e. $x_i$ is incorrectly classified\n",
    "- **Case IV:** $y_i = -ve$ and $w^Tx_i = +ve$ then $y_i * w^Tx_i < 0$ i.e. $x_i$ is incorrectly classified\n",
    "\n",
    "So the value of $w$ is chosen shuch that it gives the maximum number of points that are correctly classified points i.e. maximum value of $\\sum{y_i*w^Tx_i}$. So we need to optimize the value of $w$ and the optimal value of the normal $w$ is given by;\n",
    "\n",
    "\\begin{equation}\n",
    "w^* = argmax_w (\\sum{y_i*w^Tx_i})\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function Squashing\n",
    "\n",
    "In explanations above, we have seen, if $w$ be unit vector then, $y_iw^Tx_i$ is known as signed distance where distance = $w^Tx_i$. And our ultimite goal is to find the optimized summation of signed distance. However, the optimization shown above fails in some cases. Let's see a few examples where it fails.\n",
    "\n",
    "`Example:` Consider we have 5 positive data points, 5 negative data points and an outlier in our dataset. Let's say each data point of the same class are also separated by a unit distance from its adjescent one as shown in the figure.\n",
    "\n",
    "![Sigmoid Function Squashing](./../../assets/sigmoid-squashing.jpg)\n",
    "\n",
    "**Case I:** Let's draw a decision surface $\\pi_1$ as shown in the figure. All the positive and negative data points are unit distance away from the decision surface ($\\pi_1$), while the outlier is 100 units of distance away. Then the optimization equation above gives:\n",
    "\n",
    "$w = \\sum{y_i*w^Tx_i} = (1+1+1+1+1) + (1+1+1+1+1) - 100 = -90$\n",
    "\n",
    "**Case II:** In the same example, now let's draw a decision surface $\\pi_2$. Let's say the outlier is unit distance away from $\\pi_2$. Now this converges the above optimization equation to:\n",
    "\n",
    "$w = \\sum{y_i*w^Tx_i} = (1+2+3+4+5) + (-1-2-3-4-5) + 1 = +1$\n",
    "\n",
    "In this example, $\\pi_2$ maximizes the sum of signed distance mathematically but if we think logically $\\pi_1$ is the correct hyperplane because all the points except a single negative point are correctly classified by $\\pi_1$ while $\\pi_2$ misclassifies 5 of the data points. So, maximizing sum of the signed distance is not prone to outliers. This is where sigmoid function squashing comes into play.\n",
    "\n",
    "The idea beyound sigmoid function squashing is that as the magnitude of signed distance changes, supress the increament/decrement accordingly. The rate of supression changes according to the rate of increatement/decreament. There may be a number of functions for squashing requirements like ours, but we choose sigmoid function in particular because of following few reasons:\n",
    "\n",
    "- It is very easy to differentiate.\n",
    "- It has very nice probabilistic interpretation.\n",
    "\n",
    "![Sigmoid Function](./../../assets/sigmoid.png)\n",
    "\n",
    "*[[Source]](https://link.springer.com/chapter/10.1007/978-3-030-72280-7_7)*\n",
    "\n",
    "The optimization function now becomes:\n",
    "\n",
    "$w^* = argmax_w (\\sum{\\sigma(y_i*w^Tx_i)})$\n",
    "\n",
    "where $\\sigma = \\frac{1}{1 + e^{-z_i}}$ and $z_i = y_i*w^Tx_i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation of Objective Function\n",
    "\n",
    "If $g(x) is a monotonic function then from the basic rule of statistics we know that;\n",
    "\n",
    "$argmax_x f(x) = argmax_x g(f(x))$\n",
    "\n",
    "$argmin_x f(x) = argmin_x g(f(x))$\n",
    "\n",
    "$argmin_x f(x) = - argmax_x g(f(x))$\n",
    "\n",
    "Since $log$ is a monotonic function, we can write above optimization function as:\n",
    "\n",
    "$w^* = argmax_w (\\sum{log(\\sigma(y_i*w^Tx_i))}) = argmax_w (\\sum{log(\\frac{1}{1 + e^{-y_i*w^Tx_i}}))})$\n",
    "\n",
    "$w^* = argmin_w (\\sum{log(1 + e^{-y_i*w^Tx_i}}))$\n",
    "\n",
    "Here, $y_i$ can have two unique values $y_i = +1$ or $y_i = -1$.\n",
    "\n",
    "## Bias-Variance Tradeoff\n",
    "\n",
    "Bias-Variance Tradeoff can be done in logistic regression model using a technique known as regularization that improves the generalization of the logistic regression model. Two commonly used regularization methods are Ridge regression and Lasso regression. Before studying these two solutions, let's understand the problem first.\n",
    "\n",
    "The objective function of logistic regression is given by:\n",
    "\n",
    "$w^* = argmin_w (\\sum{log(1 + e^{-y_i*w^Tx_i}})) = argmin_w (\\sum{log(1+ e^{-z_i})})$\n",
    "\n",
    "We know the term, $e^{-z_i} \\geq 0$\n",
    "\n",
    "i.e. $log(1 + e^{-z_i}) = log(1 + +ve) \\geq log(1) \\geq 0$\n",
    "\n",
    "So the minimum possible value of $log(1 + e^{-z_i}) = 0$\n",
    "\n",
    "To make this term zero (in order to get minimum value):\n",
    "\n",
    "$log(1 + e^{-z_i}) = log(1) = 0$\n",
    "\n",
    "$1 + e^{-z_i} = 1$\n",
    "\n",
    "$z_i = +∞$\n",
    "\n",
    "So as $z_i = +∞$, $z_i$ is positive for all $i$, i.e. all the points are correctly classified i.e. even outliers are classified correctly, which is the case of overfitting. This term can further be broken into:\n",
    "\n",
    "$z_i = +∞$ which give $w_i = +-∞$ because $y_i ≠ ∞$ and $x_i ≠ ∞$.\n",
    "\n",
    "### Ridge Regression (L2 Regularization)\n",
    "\n",
    "From the equation above, we got $w_i = ∞$. This issue can be solved using Ridge Regression.\n",
    "\n",
    "Since `w` is normal to the decision surface, $w^Tw = 1$ \n",
    "\n",
    "$w^Tw = \\sum w_j^2 = ||w||_2^2$ -> L2 norm.\n",
    "\n",
    "Hence the name L2 Regularization.\n",
    "\n",
    "Ridge Regression adds a penalty term ($\\lambda$) to the objective function as shown below:\n",
    "\n",
    "$w^* = argmin_w [\\sum{log(1 + e^{-y_i*w^Tx_i}}) + \\lambda*w^Tw]$\n",
    "\n",
    "Now we cannot use $w_i = +-∞$ because this will make $w^Tw = ∞$ which in turns make the entire objective function $∞$ and not zero.\n",
    "\n",
    "**Case I:** when $\\lambda = 0 (i.e. small)$\n",
    "\n",
    "This takes us to the previous stage, where we were overfitting.\n",
    "\n",
    "**Case II:** when $\\lambda = ∞ (i.e. very high)$\n",
    "\n",
    "The entire term becomes ∞. So our training data won't matter and we get the same result irrespective of the nature of the data point. This $\\lambda$ here is the hyperparameter for logistic regression.\n",
    "\n",
    "### Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso Regression serves the same purpose as done by Ridge Regression. In addition it has an advantage of sparsity. Lasso Regression makes all the non-important features zero. In case of Ridge Regression, non-important become low but not necessarily zero.\n",
    "\n",
    "$w^* = argmin_w [\\sum{log(1 + e^{-y_i*w^Tx_i}}) + \\lambda*||w||_1]$\n",
    "\n",
    "where $||w||_1$ is the L1-Norm. Hence the name L1-Regularization.\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "Sometimes people use both L1 and L2 regularization simultaneously and this is known as elastic net. In such case we encounter two hyperparameters ($\\lambda_1$ and $\\lambda_2$). The objective function then becomes:\n",
    "\n",
    "$w^* = argmin_w [\\sum{log(1 + e^{-y_i*w^Tx_i}}) + \\lambda_1*||w||_1 + \\lambda_2*||w||_2^2]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Logistic Regression](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/logistic-regression/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
