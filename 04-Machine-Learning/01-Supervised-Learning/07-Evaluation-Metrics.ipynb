{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "An evaluation metric is a measure used to assess the performance or effectiveness of a machine learning model. It quantifies how well the model predicts or classifies the data it is given. Evaluation metrics provide objective criteria for comparing different models or assessing the quality of a model's predictions. Evaluation metrics can vary depending on the problem type.\n",
    "\n",
    "## Classificaton Tasks\n",
    "\n",
    "`Example:` Let's consider a binary classification problem where we want to predict whether an email is spam (positive) or not spam (negative). After applying our model to a test dataset of 100 emails\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a tabular representation that summarizes the performance of a classification model by counting the number of correct and incorrect predictions. It is often used to evaluate the effectiveness of a machine learning model.\n",
    "A confusion matrix typically consists of four values:\n",
    "\n",
    "- True Positive (TP): The number of positive instances correctly predicted as positive.\n",
    "- True Negative (TN): The number of negative instances correctly predicted as negative.\n",
    "- False Positive (FP): The number of negative instances incorrectly predicted as positive (Type I error).\n",
    "- False Negative (FN): The number of positive instances incorrectly predicted as negative (Type II error).\n",
    "\n",
    "A confusion matrix allows for a more detailed analysis of the model's performance beyond simple accuracy. It is particularly useful when dealing with imbalanced datasets or when the cost of false positives and false negatives varies.\n",
    "\n",
    "`Example:` Confusion matrix for the example above is as follows:\n",
    "\n",
    "| | Predicted Spam | Predicted Not Spam |\n",
    "|---|:-:|:-:|\n",
    "| Actual Spam | 70 | 5 |\n",
    "| Actual Not Spam | 10 | 15 |\n",
    "\n",
    "In this example, the confusion matrix provides a detailed breakdown of the model's predictions, showing the true positives (70), true negatives (15), false positives (10), and false negatives (5).\n",
    "\n",
    "- **When to use:** Confusion matrix is widely used for evaluating classification models, especially when dealing with imbalanced datasets or when the costs of false positives and false negatives are different.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy is a commonly used evaluation metric that measures the overall correctness of a classification model. It calculates the proportion of correctly predicted instances (TP and TN) out of the total number of instances.\n",
    "\n",
    "$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "While accuracy provides a general idea of the model's performance, it may not be suitable for imbalanced datasets, where the class distribution is uneven.\n",
    "\n",
    "`Example:` Using the same example, we calculate the accuracy of the model:\n",
    "\n",
    "$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{70 + 15}{100} = 0.85$\n",
    "\n",
    "The model achieves an accuracy of 85%, meaning that 85 out of 100 emails were correctly classified.\n",
    "\n",
    "- **When to use:** Accuracy is a commonly used metric when the class distribution is balanced, and both types of errors (false positives and false negatives) have similar costs.\n",
    "\n",
    "### Precision (False Positive Rate)\n",
    "Precision is a measure of how many of the positive predictions made by the model are actually correct. It quantifies the ability of the model to avoid false positives.\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "Precision is valuable when the cost of false positives is high, such as in medical diagnostics or fraud detection, where it is crucial to minimize false alarms.\n",
    "\n",
    "`Example:` Continuing with the previous example, we calculate the precision of the model:\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP} = \\frac{70}{70 + 10} ≈ 0.875$\n",
    "\n",
    "The precision of the model is approximately 0.875, indicating that among the emails predicted as spam, around 87.5% were actually spam.\n",
    "\n",
    "- **When to use:** Precision is useful when the cost of false positives is high, such as in medical diagnostics or fraud detection, where it is crucial to minimize false alarms.\n",
    "\n",
    "### Recall (Sensitivity or True Positive Rate)\n",
    "\n",
    "Recall, also known as sensitivity or true positive rate, measures the proportion of actual positive instances that are correctly identified by the model.\n",
    "\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "Recall is essential in scenarios where the cost of false negatives is high, such as disease diagnosis, where missing positive cases can have severe consequences.\n",
    "\n",
    "`Example:` In the same email spam classification example, we calculate the recall of the model:\n",
    "\n",
    "$Recall = \\frac{TP}{TP + FN} = \\frac{70}{70 + 5} ≈ 0.933$\n",
    "\n",
    "The recall, also known as sensitivity, is approximately 93.3%. This means that the model correctly identifies around 93.3% of the actual spam emails.\n",
    "\n",
    "- **When to use:** Recall is important when the cost of false negatives is high, such as in disease diagnosis, where missing positive cases can have severe consequences.\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "The F1 score is a harmonic mean of precision and recall. It provides a balanced measure of a model's performance, particularly when precision and recall have different priorities.\n",
    "\n",
    "$F1 Score = 2 * \\frac{Precision * Recall}{Precision + Recall}$\n",
    "\n",
    "The F1 score ranges from 0 to 1, with 1 representing the best possible performance.\n",
    "\n",
    "`Example:` Using the values from precision and recall, we calculate the F1 score:\n",
    "\n",
    "$F1 Score = 2 * \\frac{Precision * Recall}{Precision + Recall} ≈ 2 * \\frac{0.933 * 0.875}{0.933 + 0.875} ≈ 0.903$\n",
    "\n",
    "The F1 score for the model is approximately 0.903, which combines the precision and recall values into a single metric.\n",
    "When to use: The F1 score is useful when we want a balanced measure of a model's performance, particularly when precision and recall have different priorities.\n",
    "\n",
    "### ROC-AUC Curve\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of a classification model's performance across various thresholds. It shows the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) for different threshold values.\n",
    "\n",
    "The area under the ROC curve (AUC) provides a single value to measure the model's discrimination ability. A higher AUC indicates better model performance, with 1 representing a perfect classifier.\n",
    "\n",
    "The ROC-AUC curve is particularly useful when the class distribution is imbalanced or when different thresholds need to be explored for decision-making.\n",
    "\n",
    "`Example:` Suppose we have a binary classification model for predicting whether a patient has a certain disease. The ROC-AUC curve is plotted by calculating the true positive rate (sensitivity) and the false positive rate (1 - specificity) at various classification thresholds. The area under the ROC curve (AUC) is then calculated. A higher AUC value indicates better discrimination ability of the model.\n",
    "\n",
    "- **When to use:** The ROC-AUC curve is particularly useful when the class distribution is imbalanced or when different thresholds need to be explored for decision-making. It provides a comprehensive evaluation of the model's performance across various thresholds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tasks\n",
    "\n",
    "`Example:` Consider a regression problem where we want to predict housing prices based on various features. After applying our regression model to a test dataset of 100 houses, we found the result as follows:\n",
    "\n",
    "```\n",
    "Actual Prices: [250,000, 300,000, 350,000, ...]\n",
    "Predicted Prices: [260,000, 295,000, 360,000, ...]\n",
    "```\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "Mean Absolute Error is a commonly used metric for regression problems. It measures the average absolute difference between the predicted and actual values. MAE provides an easily interpretable measure of how close the predictions are to the actual values.\n",
    "\n",
    "$MAE = \\frac{1}{n} * Σ|y - ŷ|$\n",
    "\n",
    "where:\n",
    "\n",
    "- `n` is the number of instances\n",
    "- `y` represents the actual values\n",
    "- `ŷ` represents the predicted values\n",
    "\n",
    "MAE is relatively simple to understand and compute but does not consider the squared errors, potentially making it less sensitive to large errors.\n",
    "\n",
    "`Example:` For the problem above, we calculate the MAE as follows:\n",
    "\n",
    "$MAE = \\frac{|250,000 - 260,000| + |300,000 - 295,000| + |350,000 - 360,000| + ...}{100}$\n",
    "\n",
    "The MAE provides an average of the absolute differences between the actual and predicted values. In this example, it measures the average difference between the predicted and actual housing prices.\n",
    "\n",
    "- **When to use:** MAE is a common metric for evaluating regression models and is suitable when you want to understand the average magnitude of the errors without considering their direction.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "Mean Squared Error is another popular metric for regression evaluation. It measures the average of the squared differences between the predicted and actual values. MSE gives more weight to larger errors compared to MAE.\n",
    "\n",
    "$MSE = \\frac{1}{n} * Σ(y - ŷ)^2$\n",
    "\n",
    "MSE provides a more comprehensive measure of the model's performance by considering both small and large errors. However, it is not directly interpretable in the original scale of the target variable.\n",
    "\n",
    "`Example:` Using the same housing price regression example, we calculate the MSE:\n",
    "\n",
    "$MSE = \\frac{(250,000 - 260,000)^2 + (300,000 - 295,000)^2 + (350,000 - 360,000)^2 + ...}{100}$\n",
    "\n",
    "MSE calculates the average of the squared differences between the predicted and actual values. It penalizes larger errors more heavily than MAE.\n",
    "\n",
    "- **When to use:** MSE is widely used in regression tasks and is beneficial when you want to emphasize larger errors and penalize them more compared to smaller errors.\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE is derived from MSE and is widely used due to its interpretability in the original scale of the target variable. It represents the square root of the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{n} * Σ(y - ŷ)^2}$\n",
    "\n",
    "RMSE shares the same properties as MSE but provides a more easily understandable and interpretable metric.\n",
    "\n",
    "`Example:` Based on the previous example, we calculate the RMSE as follows:\n",
    "\n",
    "$RMSE = sqrt(MSE)$\n",
    "\n",
    "RMSE provides the square root of the MSE, giving a measure of the average magnitude of the errors in the original unit of the target variable (e.g., dollars in the housing price example).\n",
    "\n",
    "- **When to use:** RMSE is a commonly used metric for regression problems and is especially useful when you want to interpret the errors in the original unit of the target variable.\n",
    "\n",
    "### R-squared Error (Coefficient of Determination)\n",
    "\n",
    "R-squared (R²) is a statistical metric that represents the proportion of variance in the target variable explained by the regression model. It indicates how well the model fits the data and ranges from 0 to 1. A higher R² value indicates a better fit.\n",
    "\n",
    "$R² = 1 - \\frac{SSE}{SST}$\n",
    "\n",
    "where:\n",
    "\n",
    "- `SSE` is the sum of squared residuals $(predicted values - actual values)^2$\n",
    "- `SST` is the total sum of squares $(actual values - mean of actual values)^2$\n",
    "\n",
    "R-squared measures the goodness of fit but does not consider the complexity of the model or the number of predictors.\n",
    "\n",
    "`Example:` In the housing price regression example, we can calculate the R-squared error as follows:\n",
    "\n",
    "$SST = \\sum{((Actual Prices - mean(Actual prices))^2}$\n",
    "\n",
    "Calculate the residual sum of squares (SSE):\n",
    "\n",
    "$SSE = \\sum{((Actual Prices - Predicted Prices))^2}$\n",
    "\n",
    "Calculate the R-squared error:\n",
    "\n",
    "$R² = 1 - \\frac{SSE}{SST}$\n",
    "\n",
    "The R-squared error measures the proportion of the variance in the target variable that can be explained by the regression model. It ranges from 0 to 1, with 1 indicating that the model explains all the variability in the target variable.\n",
    "\n",
    "- **When to use:** R-squared error is commonly used to assess the goodness of fit of a regression model. It provides an indication of how well the model fits the data.\n",
    "\n",
    "### Adjusted R-squared Error\n",
    "\n",
    "Adjusted R-squared takes into account the number of predictors in the model to address the potential issue of overfitting. It penalizes complex models with more predictors, providing a more accurate assessment of the model's fit.\n",
    "\n",
    "$Adjusted R² = 1 - (1 - R²) * \\frac{n - 1}{n - p - 1}$\n",
    "\n",
    "where:\n",
    "\n",
    "- `n` is the number of instances\n",
    "- `p` is the number of predictors\n",
    "\n",
    "Adjusted R-squared adjusts R-squared for the degrees of freedom, providing a more reliable measure of the model's performance, especially when comparing models with a different number of predictors.\n",
    "\n",
    "- **When to use:** Adjusted R-squared is useful when comparing models with different numbers of predictors. It helps in selecting the best model by considering both the goodness of fit and the number of predictors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
