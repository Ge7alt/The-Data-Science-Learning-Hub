{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "A loss function is typically used in the context of training a machine learning model on a single training example. It measures the error or discrepancy between the predicted output of the model and the true output for that particular training example. The goal is to minimize this loss function during the training process, which involves adjusting the model's parameters.\n",
    "\n",
    "Let's denote the predicted output of the model as $ȳ$ and the true output as $y$. The loss function, denoted by $L$, takes these values as input and computes the error. Commonly used loss functions include mean squared error (MSE), binary cross-entropy, categorical cross-entropy, etc. Here's an example of the mean squared error loss function:\n",
    "\n",
    "$L(ȳ, y) = (ȳ - y)^2$\n",
    "\n",
    "The loss function quantifies how well the model is performing on a single training example. The objective is to minimize the loss function by adjusting the model's parameters to improve its predictions.\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "A cost function, on the other hand, is used in the context of training a machine learning model on an entire training dataset, consisting of multiple training examples. The cost function is an aggregate or average of the individual loss functions computed for each training example. It provides a measure of how well the model is performing on the entire dataset.\n",
    "\n",
    "If we have n training examples, we can denote the predicted outputs for all the examples as $ȳ_1, ȳ_2, ..., ȳ_n$, and the true outputs as $y_1, y_2, ..., y_n$. The cost function, denoted by J, takes these values as input and computes the overall error or cost. It is typically the average (or sum) of the individual loss functions:\n",
    "\n",
    "$J(ȳ_1, ȳ_2, ..., ȳ_n, y_1, y_2, ..., y_n) = (1/n) * Σ L(ȳ_i, y_i)$\n",
    "\n",
    "The goal is to minimize the cost function by adjusting the model's parameters during the training process. This involves considering the collective performance of the model on all the training examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize a function iteratively. It is commonly applied in machine learning and deep learning to update the parameters of a model in order to minimize a cost or loss function. The goal is to find the values of the parameters that correspond to the minimum value of the function.\n",
    "\n",
    "Gradient descent works by taking steps proportional to the negative of the gradient of the function at the current point. The gradient is the vector of partial derivatives of the function with respect to each parameter. By following the direction opposite to the gradient, we iteratively move towards the minimum of the function.\n",
    "\n",
    "The steps of the gradient descent algorithm can be summarized as follows:\n",
    "\n",
    "- Initialize the parameters with some initial values.\n",
    "    - `Example:` Let's say we initialize the weight `w` as 0.5.\n",
    "    \n",
    "- Calculate the gradient of the function with respect to the parameters.\n",
    "    - `Example:` Example: Assuming we have a loss function `L(w)` that depends on the weight `w`, we need to compute the derivative or gradient of the loss function with respect to `w`. Let's say the gradient is given by:\\\n",
    "    $dL/dw = 2w$\n",
    "- Update the parameters by taking a step in the opposite direction of the gradient.\n",
    "    - `Example:` To update the weight `w`, we subtract a fraction of the gradient from the current value of `w`. This fraction is determined by the learning rate (`α`). Let's say the learning rate is 0.1. The update equation becomes:\\\n",
    "    $w = w - α * dL/dw$\\\n",
    "    $w = w - 0.1 * 2w$\n",
    "- Repeat steps 2 and 3 until convergence (when the parameters no longer change significantly or the maximum number of iterations is reached)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "Stochastic Gradient Descent (SGD) is a variant of the standard Gradient Descent (GD) algorithm. It is often preferred over GD in certain scenarios due to its unique advantages.\n",
    "\n",
    "In standard Gradient Descent, the model parameters are updated by computing the average gradient over the entire training dataset. This involves evaluating the gradient for each training example, summing them up, and then taking the average. The updated parameters are then moved in the opposite direction of this averaged gradient.\n",
    "\n",
    "On the other hand, Stochastic Gradient Descent updates the parameters based on a single training example or a small random subset of examples, often called a mini-batch. Instead of computing the average gradient over the entire dataset, SGD computes the gradient for each individual example or mini-batch and updates the parameters accordingly. This process is repeated for multiple iterations or until convergence.\n",
    "\n",
    "![Stochastic Gradient Descent](./../../assets/sgd.jpg)\n",
    "\n",
    "### Pros of SGD\n",
    "- **Efficiency for Large Datasets:** SGD handles large datasets more efficiently than GD, as it processes individual examples or small batches at a time. This reduces memory requirements and speeds up the training process.\n",
    "- **Better Generalization:** SGD's stochastic nature can help prevent overfitting, as it introduces noise that allows the model to explore different regions of the parameter space and generalize better to unseen data.\n",
    "\n",
    "### Cons of SGD:\n",
    "- **Noisy Updates:** SGD's randomness and frequent updates can introduce noise in the optimization process, making the convergence path more erratic and potentially causing slower convergence.\n",
    "- **Sensitivity to Learning Rate:** The learning rate in SGD needs to be carefully chosen. A high learning rate can cause overshooting and instability, while a low learning rate can slow down convergence.\n",
    "- **May Not Reach Global Minima:** Due to its stochastic nature, SGD may not reach the global minimum of the loss function, especially in non-convex optimization problems. It may find good local minima but cannot guarantee finding the absolute best solution.\n",
    "\n",
    "But, sometimes the loss/cost function curve cannot be as simple as we discussed earlier. Sometimes it may have multiple local minima or saddle points like shown in the figure below. This will create a confusion and our model may converge to a local minimum or a saddle point but not to a global minima. So our model may never arrive to the global optimization point.\n",
    "\n",
    "![Local Minima and Saddle Points](./../../assets/local-minima.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) with Momentum\n",
    "\n",
    "SGD with momentum enhances the basic Stochastic Gradient Descent (SGD) algorithm by introducing a momentum term `v` that accumulates the previous gradients. This momentum term influences the direction and speed of parameter updates, leading to faster convergence and improved optimization performance.\n",
    "\n",
    "Here's a step-by-step explanation of how SGD with momentum works:\n",
    "\n",
    "**Initialization:**\n",
    "- Initialize the parameters, such as weights ($w$) and biases ($b$), with some initial values.\n",
    "- Initialize the momentum term ($v_o$) to zero, where $v_o$ is the momentum term at $0^{th}$ iteration.\n",
    "- Define the learning rate ($α$) and the momentum coefficient ($γ$).\n",
    "\n",
    "**Compute Gradient:**\n",
    "- Randomly select a mini-batch of training samples $(X, y)$.\n",
    "- Calculate the gradient of the loss function with respect to the parameters using the mini-batch.\n",
    "- Denote the gradient as $g_t$, indicating it to be the gradient at iteration `t`.\n",
    "\n",
    "**Update Momentum:**\n",
    "- Update the momentum term using the momentum coefficient ($γ$) and the previous momentum value ($v_{t-1}$).\n",
    "- The update equation for the momentum term is:\n",
    "    - $v_t = γ * v_{t-1} + α * g_t$\n",
    "\n",
    "    The momentum term accumulates the previous gradients scaled by the momentum coefficient. It adds a fraction of the previous momentum to the current gradient, reinforcing the direction of the previous updates. This accumulation allows the algorithm to gain momentum and continue moving in a consistent direction.\n",
    "\n",
    "**Update Parameters:**\n",
    "- Update the parameters using the momentum term and the learning rate (`α`).\n",
    "- The update equation for the parameters is:\n",
    "\n",
    "    $w_t = w_{t-1} - v_t \\\\\n",
    "    w_t = w_{t-1} - (γ * v_{t-1} + α * g_t)$\n",
    "\n",
    "    **Case:** When $γ=0; \\quad w_t = w_{t-1} - α * g_t$ \n",
    "\n",
    "    The parameters are updated by subtracting the momentum term from the current values. The momentum term acts as a velocity or accumulated force that guides the updates in a particular direction. The learning rate scales the momentum term, determining the step size in the parameter space.\n",
    "\n",
    "**Repeat Steps 2-4:**\n",
    "- Repeat Steps 2-4 for multiple iterations or until convergence is achieved.\n",
    "\n",
    "So the final momentum term looks like:\n",
    "\n",
    "$v_t = α*g_t + γ*α*g_{t-1} + γ^2*α*g_{t-2} + γ^3*α*g_{t-3} + ...$\n",
    "\n",
    "This equation shows that to calculate current momentum term we are using all the preious terms. Hence, the name momentum.\n",
    "\n",
    "![SGD with Momentum](./../../assets/sgd-momentum.gif)\n",
    "\n",
    "By incorporating momentum, SGD with momentum helps overcome some limitations of basic SGD. Let's look into some of them:\n",
    "\n",
    "**Navigating Flat Regions:** Sometimes, the loss function may have flat regions or plateaus or saddle points. In these regions, the gradients become very small, and standard SGD may get stuck. However, SGD with momentum allows the algorithm to bypass these flat regions. The accumulated momentum acts as a driving force, pushing the algorithm through the flat regions and preventing it from getting trapped.\n",
    "\n",
    "**Overcoming Local Curvature:** Local minima can have varying curvatures. In regions with high curvature, standard SGD might struggle because the diminishing gradients make progress slow. With SGD with momentum, the accumulated momentum helps the algorithm overcome the influence of local curvature. It provides an additional push that allows the algorithm to move across the curvatures and explore other areas of the parameter space, increasing the chances of finding a better solution beyond local minima.\n",
    "\n",
    "**Dampening Oscillations:** Oscillations around local minima can hinder convergence. SGD with momentum helps dampen these oscillations by smoothing out the updates. The accumulated momentum provides stability and consistency to the updates, preventing erratic back-and-forth movements that might hinder progress towards the optimal solution.\n",
    "\n",
    "As a result of $-α*g_t$ and $γ*v_{t-1}$ we will neither move in the direction of gradient nor in the direction of momentum but in the direction of resultant $w_t$ as shown in the figure below (in the first figure of the comaprison between SGD with momentum with NAG)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "Nesterov Accelerated Gradient differs from traditional momentum methods only by calculating the gradient ahead of the current position otherwise they represent the same information. NAG introduces the concept of a \"look ahead\" gradient. Instead of using the current position's gradient for updating the parameters, NAG calculates the gradient at a \"look ahead\" position. This \"look ahead\" position is determined by applying a momentum term to the current parameters, resulting in faster convergence.\n",
    "\n",
    "$\n",
    "w_t = w_{t-1} - (γ * v_{t-1} + α * g_t') \\\\\n",
    "g_t' = (\\frac{dLoss}{dw})_{w_{t-1} - γ*v_{t-1}}\n",
    "$\n",
    "\n",
    "![Nesterov Accelerated Gradient](./../../assets/nag.jpg)\n",
    "\n",
    "The key advantage of NAG is that it incorporates information about the future gradient, which helps prevent overshooting the optimal solution. By considering the \"look ahead\" gradient, NAG adjusts the parameter updates accordingly, ensuring more precise movement towards the minimum. This can result in faster convergence compared to standard gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Gradient (AdaGrad)\n",
    "AdaGrad adapts the learning rate to each parameter based on the historical gradients. It gives more importance to parameters with smaller gradients by increasing their learning rate. Conversely, parameters with larger gradients will have a smaller effective learning rate. This adaptive learning rate scheme allows AdaGrad to perform well in scenarios where different parameters require different learning rates for optimal convergence.\n",
    "\n",
    "$\n",
    "w_t = w_{t-1} - α_t'*g_t \\\\\n",
    "α_t' = \\frac{α}{\\sqrt{G_t + \\epsilon}} \\\\\n",
    "G_t = \\sum_{i=1}^t g_i^2\n",
    "$\n",
    "\n",
    "where $G$ is called the `accumulator` which keeps track of the sum of squared gradients for a given parameter.\n",
    "\n",
    "Since we are summing up $g_i^2$, as $t (↑) ⇒ G_t (↑) ⇒ α_t' (↓)$\n",
    "\n",
    "One of the advantages of AdaGrad is its ability to automatically adjust the learning rate, reducing the need for manual tuning. However, a potential drawback is that the accumulated squared gradients keep growing over time, which can lead to overly small learning rates in later stages of training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaDelta\n",
    "AdaDelta improves upon AdaGrad by addressing the unbounded accumulation of squared gradients. Instead of accumulating all past gradients, AdaDelta uses an exponentially decaying average, allowing it to consider only a limited history. This helps prevent the learning rate from becoming overly small over time.\n",
    "\n",
    "$\n",
    "EDA[g^2]_t = γ * EDA[g^2]_{t-1} + (1 - γ) * g^2_{t-1} \\\\\n",
    "α_t' = \\sqrt{\\frac{EDA[Δw_{t-1}^2] + \\epsilon}{EDA[g_{t-1}^2] + \\epsilon}} \\\\\n",
    "Δw_t = - α_t' * g_t \\\\\n",
    "w_t = w_{t-1} + Δw_t\n",
    "$\n",
    "\n",
    "Additionally, AdaDelta introduces the accumulator update 'Δw', which further scales the updates based on the ratio of the root mean square of previous updates to the root mean square of the gradients. This allows the algorithm to adaptively adjust the step size for each parameter, improving optimization performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RmsProp\n",
    "\n",
    "RMSprop improves upon AdaGrad by introducing the exponentially decaying moving average of squared gradients (`v`). This limits the influence of older gradients and prevents the unbounded accumulation of squared gradients. The moving average provides a better estimate of the variance of the gradients and allows the algorithm to adapt the learning rate effectively.\n",
    "\n",
    "$\n",
    "v_t = γ * v_{t-1} + (1 - γ) * g_t^2 \\\\\n",
    "α_t' = \\frac{α_t}{\\sqrt{v_t + \\epsilon}} \\\\\n",
    "w_t = w_t-1 - α_t'*g_t\n",
    "$\n",
    "\n",
    "The choice of the decay rate (`γ`) is crucial. Typically, a value around 0.9 works well in practice. It determines how much weight is given to the most recent gradients compared to the historical ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Moment Estimation (Adam)\n",
    "\n",
    "Adam combines the benefits of RMSprop's adaptive learning rates and momentum's accelerated updates. The first moment estimate `m` acts as momentum, preventing the algorithm from getting stuck in flat regions, while the second moment estimate `v` adapts the learning rates for each parameter based on the variance of the gradients. The bias correction step compensates for the initialization bias at the beginning of training.\n",
    "\n",
    "$\n",
    "m_t = γ₁ * m_{t-1} + (1 - γ₁) * g_t \\\\\n",
    "v_t = γ₂ * v_{t-1} + (1 - γ₂) * g_t^2 \\\\\n",
    "\\hat m_t = \\frac{m_t}{1 - γ_{1t}} \\\\\n",
    "\\hat v_t = \\frac{v_t}{1 - γ_{2t}} \\\\\n",
    "α_t' = \\frac{α_t'}{\\sqrt{\\hat v_t + \\epsilon}} \\\\\n",
    "w_{t-1} = w_t - a_t'*\\hat m_t\n",
    "$\n",
    "\n",
    "The hyperparameters to tune in Adam are the decay rates `γ₁` and `γ₂`, typically set to 0.9 and 0.999, respectively. The learning rate and epsilon also need to be carefully chosen based on the specific problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Gradient Descent with Momentum](https://gbhat.com/machine_learning/gradient_descent_with_momentum.html)\n",
    "- [Stochastic Gradient Descent with momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)\n",
    "- [Nesterov Accelerated Gradient](https://www.codingninjas.com/codestudio/library/nesterov-accelerated-gradient)\n",
    "- [AdaDelta](https://d2l.ai/chapter_optimization/adadelta.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
