{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Techniques in Deep Neural Networks\n",
    "\n",
    "Optimization Techniques in Deep Neural Networks refers to a set of methods and strategies used to improve the training process and enhance the performance of deep neural networks. Deep neural networks are complex models with multiple layers, and training them involves finding the optimal set of weights and biases that minimize a given loss function.\n",
    "\n",
    "Optimization techniques aim to overcome challenges that arise during the training process, such as the vanishing or exploding gradient problem, slow convergence, overfitting, and difficulties in finding the global optimum. These techniques involve adjusting the model's parameters iteratively to optimize the network's performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient\n",
    "\n",
    "Before talking about the optimization techniques let's understand the problem with the neural networks studied so far. Vanishing Gradient is one of such problems. The vanishing gradient problem is a phenomenon that can occur during the training of Multi-Layer Perceptron (MLP) neural networks. It refers to the situation where the gradients of the loss function with respect to the weights of the network become extremely small as they are backpropagated from the output layer to the earlier layers of the network.\n",
    "\n",
    "In an MLP, the backpropagation algorithm is commonly used to update the network weights based on the calculated gradients. During backpropagation, the gradients are computed by recursively applying the chain rule of calculus, multiplying the gradients of each layer to compute the gradients of the preceding layers. However, in deep networks with many layers, the multiplication of these gradients can cause the gradient values to shrink exponentially as they propagate backward through the layers.\n",
    "\n",
    "As seen in previous neural network, one of the gradient calculation looked like the one below:\n",
    "\n",
    "$\n",
    "\\frac{∂L}{∂w_{11}^1} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂O_{11}} * \\frac{∂O_{11}}{∂w_{11}^1} = 0.2*0.1*0.05 (let) = 10^{-3} \\\\\n",
    "w_{updated} = 2.5(let) - (η=1)*10^{-3} = 2.499 \\quad\\quad [let\\ η=1]\n",
    "$\n",
    "\n",
    "When the gradients become very small, it becomes difficult for the network to learn effectively because the weight updates become insignificant, and the network may struggle to converge to an optimal solution. This problem is especially pronounced in deep networks with many layers, where the vanishing gradients can make it challenging to train the earlier layers effectively, as they receive increasingly small gradient updates.\n",
    "\n",
    "The problem of vanishing gradient can be solved to certain extent using ReLU as the activation function as;\n",
    "\n",
    "$\n",
    "ReLU(x)=max(0,x) \\\\\n",
    "Derivative of ReLU(x)  = \\begin{cases}\n",
    "    0, & \\text{if } x < 0 \\\\\n",
    "    1, & \\text{if } x > 0 \\\\\n",
    "    undefined, & \\text{if } x = 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Several techniques have been developed to mitigate the vanishing gradient problem, including:\n",
    "\n",
    "- **Initialization techniques:** Properly initializing the weights of the network can alleviate the vanishing gradient problem. Techniques like Xavier initialization and He initialization help in setting the initial weights in a way that avoids extreme saturation of the activation functions.\n",
    "- **Activation functions:** Using activation functions that do not suffer from the vanishing gradient problem can help. Rectified Linear Units (ReLU) and its variants, such as Leaky ReLU and Parametric ReLU, have gradients that do not diminish for positive inputs, which can alleviate the problem to some extent.\n",
    "- **Weight regularization:** Applying regularization techniques, such as L1 or L2 regularization, can prevent the network from becoming too sensitive to small weight updates, potentially mitigating the vanishing gradient problem.\n",
    "- **Residual connections:** Architectures like Residual Neural Networks (ResNets) introduce skip connections that allow gradients to bypass multiple layers, which can help alleviate the vanishing gradient problem.\n",
    "- **Batch normalization:** Applying batch normalization to the network's activations can help stabilize the distribution of values and reduce the impact of vanishing gradients.\n",
    "\n",
    "## Exploding Gradient\n",
    "\n",
    "The exploding gradient problem is another issue that can occur during the training of deep neural networks. It is the opposite of the vanishing gradient problem, where the gradients grow exponentially as they propagate backward through the layers, resulting in unstable updates and difficulties in training the network effectively.\n",
    "\n",
    "When the gradients become too large, it can lead to unstable weight updates, causing the network to diverge or fail to converge to an optimal solution. This problem is particularly prominent in recurrent neural networks (RNNs) and networks with recurrent connections, as the gradients can accumulate over time steps.\n",
    "\n",
    "The exploding gradient problem is often caused by factors such as:\n",
    "\n",
    "- **Improper initialization:** Initializing the weights of the network with large values can contribute to the exploding gradient problem. For example, if the weights are initialized randomly with values that are too large, the gradients can quickly become large during backpropagation.\n",
    "- **High learning rates:** Using a learning rate that is too high can exacerbate the exploding gradient problem. Large learning rates cause the gradients to grow rapidly, leading to unstable weight updates.\n",
    "- **Activation functions:** Certain activation functions, such as the hyperbolic tangent function, can amplify the gradients when inputs are large. This can contribute to the exploding gradient problem, particularly when combined with the factors mentioned above.\n",
    "\n",
    "Several techniques can be employed to address the exploding gradient problem:\n",
    "\n",
    "- **Gradient clipping:** Gradient clipping is a technique where the gradients are scaled down if they exceed a predefined threshold. This ensures that the gradients remain within a reasonable range, preventing them from growing uncontrollably.\n",
    "- **Weight regularization:** Regularization techniques, such as L2 regularization, can help mitigate the exploding gradient problem. By adding a penalty term to the loss function, weights are discouraged from growing excessively, which indirectly limits the magnitude of gradients.\n",
    "- **Gradient normalization:** Gradient normalization techniques, such as gradient scaling or gradient rescaling, aim to keep the magnitude of gradients within a certain range. This prevents the gradients from becoming too large and causing instability.\n",
    "- **Initialization strategies:** Proper weight initialization techniques, such as Xavier or He initialization, can help prevent the exploding gradient problem. Initializing the weights with appropriate scaling factors ensures that the gradients are not excessively amplified during backpropagation.\n",
    "- **Lowering learning rates:** Reducing the learning rate can also help mitigate the exploding gradient problem. Smaller learning rates allow for more stable weight updates and prevent the gradients from growing too large.\n",
    "\n",
    "By employing these techniques, the exploding gradient problem can be mitigated, and the training of deep neural networks can become more stable and effective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "\n",
    "Weight initialization refers to the process of assigning initial values to the weights in a neural network before training begins. The weights in a neural network are the learnable parameters that determine the strength of connections between neurons. Proper initialization of these weights is crucial because it can affect the convergence speed and overall performance of the network.  If the weights are initialized improperly, the network may have difficulty converging to a good solution or get stuck in a poor local minimum.\n",
    "\n",
    "There are several weight initialization techniques. Let me explain a few of them:\n",
    "\n",
    "1. **Zero Initialization:** This method initializes all the weights to zero. While it's simple to implement, it's generally not recommended because it leads to symmetric gradients for all neurons in a layer. As a result, all neurons in a given layer would end up learning the same features and the network would fail to learn complex representations.\n",
    "2. **Random Initialization:** In this technique, weights are randomly initialized from a uniform or normal distribution. The idea is to break the symmetry and provide each neuron with a unique starting point. However, it's important to keep the scale of the initial weights in check. If the weights are too large, it can lead to exploding gradients, and if they are too small, it can cause vanishing gradients. Xavier initialization and He initialization are two popular methods based on random initialization.\n",
    "3. **Xavier Initialization:** Proposed by Xavier Glorot and Yoshua Bengio, Xavier initialization sets the initial weights from a Gaussian distribution with zero mean and a variance that depends on the number of input and output connections of the layer. It's designed to ensure that the variance of the inputs and outputs of each layer is roughly the same, facilitating the flow of gradients during backpropagation.\n",
    "4. **He Initialization:** He initialization, proposed by Kaiming He et al., is similar to Xavier initialization but takes into account only the number of input connections. It initializes the weights from a Gaussian distribution with zero mean and a variance of 2/n, where n is the number of inputs. He initialization is commonly used in deep neural networks, especially those that use the Rectified Linear Unit (ReLU) activation function.\n",
    "5. **LeCun Initialization (also known as \"Lecun Normal\" or \"Lecun Uniform\"):** This initialization technique, introduced by Yann LeCun, sets the weights from a Gaussian or uniform distribution with a variance that depends on the number of inputs. It takes into account both the number of inputs and the nonlinearity of the activation function, which can be useful in certain types of networks, such as convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Initialization: \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Random Initialization: \n",
      " [[ 0.16684535  2.20057136  0.1775915 ]\n",
      " [ 0.17855461 -0.64829663  0.24534222]]\n",
      "\n",
      "Xavier Initialization: \n",
      " [[-0.2442879  -0.70450867 -0.25008848]\n",
      " [ 1.23657489  0.692137   -0.1583736 ]]\n",
      "\n",
      "He Initialization: \n",
      " [[-0.88873158  0.43819829 -1.5367056 ]\n",
      " [ 0.2609265  -0.53617472 -0.25676172]]\n",
      "\n",
      "LeCun Initialization: \n",
      " [[ 0.30646619  0.49589021  0.47733979]\n",
      " [ 0.5624127   0.4568826  -0.17367136]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "shape = (2, 3)  # Shape of the weight matrix for a given layer\n",
    "\n",
    "def initialize_zeros(shape):   # Zero Initialization\n",
    "    return np.zeros(shape)\n",
    "\n",
    "def initialize_random(shape):  # Random Initialization\n",
    "    return np.random.randn(*shape)\n",
    "\n",
    "def initialize_xavier(shape):  # Xavier Initialization\n",
    "    fan_in, fan_out = shape[0], shape[1]\n",
    "    variance = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return np.random.randn(*shape) * variance\n",
    "\n",
    "def initialize_he(shape):  # He Initialization\n",
    "    fan_in = shape[0]\n",
    "    variance = np.sqrt(2.0 / fan_in)\n",
    "    return np.random.randn(*shape) * variance\n",
    "\n",
    "def initialize_lecun(shape):  # LeCun Initialization\n",
    "    fan_in = shape[0]\n",
    "    variance = np.sqrt(1.0 / fan_in)\n",
    "    return np.random.randn(*shape) * variance\n",
    "\n",
    "print(\"Zero Initialization: \\n\", initialize_zeros(shape))\n",
    "print(\"\\nRandom Initialization: \\n\", initialize_random(shape))\n",
    "print(\"\\nXavier Initialization: \\n\", initialize_xavier(shape))\n",
    "print(\"\\nHe Initialization: \\n\", initialize_he(shape))\n",
    "print(\"\\nLeCun Initialization: \\n\", initialize_lecun(shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout is a regularization technique commonly used in deep learning to prevent overfitting and improve the generalization of neural networks. It randomly sets a fraction of input units to 0 at each training iteration, which helps the network to learn redundant representations and reduces the reliance on specific features.\n",
    "\n",
    "![Dropout](./../../assets/dropout.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X:\n",
      " [[-0.29970224  1.42216279  0.82231607  1.12338201  0.59489387]\n",
      " [ 0.54347447 -1.4738124   0.59482871  1.28841826  1.43048417]\n",
      " [ 1.17510505  1.24199944  2.8127914  -0.77245797 -0.33358954]\n",
      " [ 0.62784865  0.43827907 -0.9939185   0.78416389 -0.95768407]]\n",
      "\n",
      "Output with dropout:\n",
      " [[-0.3746278   1.77770349  1.02789509  0.          0.        ]\n",
      " [ 0.67934309 -0.          0.74353588  0.          0.        ]\n",
      " [ 1.46888132  1.5524993   3.51598925 -0.96557247 -0.41698692]\n",
      " [ 0.          0.54784884 -1.24239812  0.98020486 -1.19710509]]\n",
      "\n",
      "Gradient after dropout:\n",
      " [[-0.09730169 -3.00456475  1.29982597  0.         -0.        ]\n",
      " [-0.70048436  0.          0.15534742 -0.          0.        ]\n",
      " [ 1.3991457  -0.79931727  0.68408411 -1.55289726 -0.60194303]\n",
      " [-0.          3.23081862 -0.69971464  1.2343028  -0.24219732]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dropout_forward(X, dropout_rate):\n",
    "    mask = np.random.rand(*X.shape) < (1 - dropout_rate)\n",
    "    out = X * mask / (1 - dropout_rate)\n",
    "    cache = (mask, dropout_rate)\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    mask, dropout_rate = cache\n",
    "    dX = dout * mask / (1 - dropout_rate)\n",
    "    return dX\n",
    "\n",
    "# Forward pass with dropout\n",
    "dropout_rate = 0.2\n",
    "X = np.random.randn(4, 5)  # Example hidden layer output\n",
    "out, cache = dropout_forward(X, dropout_rate)\n",
    "\n",
    "# Backward pass with dropout\n",
    "dout = np.random.randn(*out.shape)  # Example gradient from subsequent layer\n",
    "dX = dropout_backward(dout, cache)\n",
    "\n",
    "print(\"Original X:\\n\", X)\n",
    "print(\"\\nOutput with dropout:\\n\", out)\n",
    "print(\"\\nGradient after dropout:\\n\", dX)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we generate a random input `X` representing the output of a hidden layer. We set the dropout rate to 0.2, indicating that we want to drop 20% of the units during training.\n",
    "\n",
    "During the forward pass, we call `dropout_forward` with `X` and the dropout rate. It returns the output after dropout (`out`) and the cache (`cache`) containing the mask and dropout rate.\n",
    "\n",
    "During the backward pass, we generate a random gradient `dout` representing the gradient from the subsequent layer. We call `dropout_backward` with `dout` and the `cache` to obtain the gradient after dropout (`dX`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch normalization is a technique used in deep learning to improve the training process and overall performance of neural networks. It addresses the internal covariate shift problem, which occurs as the distribution of inputs to each layer of a neural network changes during training. This shift makes it challenging for the network to learn effectively.\n",
    "\n",
    "Batch normalization solves the covariate shift problem by normalizing the inputs of each layer to have zero mean and unit variance. This normalization is performed over mini-batches of training examples, hence the name \"batch normalization.\" By maintaining stable input distributions, batch normalization helps the network converge faster and improves its generalization capabilities.\n",
    "\n",
    "Additionally, batch normalization has regularization effects, reducing the need for other regularization techniques such as dropout or weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNormalization:\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "        self.x_normalized = None\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if self.mean is None:\n",
    "            self.mean = np.mean(x, axis=0)\n",
    "            self.var = np.var(x, axis=0)\n",
    "\n",
    "        if training:\n",
    "            x_normalized = (x - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "            self.x_normalized = x_normalized\n",
    "\n",
    "            if self.gamma is None:\n",
    "                self.gamma = np.ones_like(x[0])\n",
    "                self.beta = np.zeros_like(x[0])\n",
    "\n",
    "            out = self.gamma * x_normalized + self.beta\n",
    "        else:\n",
    "            x_normalized = (x - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "            out = self.gamma * x_normalized + self.beta\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx_normalized = dout * self.gamma\n",
    "        dx = (1.0 / len(dout)) * (1.0 / np.sqrt(self.var + self.epsilon)) * (\n",
    "                len(dout) * dx_normalized - np.sum(dx_normalized, axis=0)\n",
    "                - self.x_normalized * np.sum(dx_normalized * self.x_normalized, axis=0))\n",
    "        dgamma = np.sum(dout * self.x_normalized, axis=0)\n",
    "        dbeta = np.sum(dout, axis=0)\n",
    "\n",
    "        self.gamma -= dgamma\n",
    "        self.beta -= dbeta\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BatchNormalization` class encapsulates the batch normalization functionality. Here's a breakdown of the implementation:\n",
    "\n",
    "- In the constructor, `epsilon` is a small constant added to the variance to avoid division by zero. The class attributes `gamma`, `beta`, `mean`, `var`, and `x_normalized` are initialized to None.\n",
    "- In the `forward` method, `x` represents the input to the batch normalization layer, and `training` determines whether the network is in training mode or not. If it's the first forward pass, the mean and variance of the input are computed and stored.\n",
    "- If in training mode, the input `x` is normalized using the mean and variance. The `gamma` and `beta` parameters, which act as learnable parameters, scale and shift the normalized input, respectively.\n",
    "- If not in training mode, the stored mean and variance are used for normalization. The `gamma` and `beta` parameters are applied as before.\n",
    "- The `backward` method performs the backward pass. The gradients with respect to the normalized input (`dx_normalized`), the input (`dx`), `gamma`, and `beta` are computed using the chain rule.\n",
    "- The updated gradients of `gamma` and `beta` are subtracted from their corresponding attributes.\n",
    "\n",
    "Now, let's see an example of how to use the `BatchNormalization` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of BatchNormalization\n",
    "bn = BatchNormalization()\n",
    "\n",
    "# Assume we have an input tensor x and its gradient dout\n",
    "x = np.random.randn(100, 10)  # Example input\n",
    "dout = np.random.randn(100, 10)  # Example gradient from subsequent layer\n",
    "\n",
    "# Forward pass\n",
    "out = bn.forward(x, training=True)\n",
    "\n",
    "# Backward pass\n",
    "dx = bn.backward(dout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we instantiate a `BatchNormalization` object and pass an input tensor `x` and its gradient `dout`. We perform the forward pass by calling `forward` with `x` and the training flag set to `True`.\n",
    "\n",
    "During the backward pass, we call `backward` with the gradient `dout`. It computes the gradients and returns `dx`, the gradient with respect to the input.\n",
    "\n",
    "You can integrate the `BatchNormalization` class into your neural network implementation by calling the `forward` method after the activation of each layer and the `backward` method during the backward pass."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning, including deep neural networks. It refers to the tradeoff between the model's ability to fit the training data (low bias) and its ability to generalize well to unseen data (low variance). Overfitting and underfitting are two scenarios related to this tradeoff.\n",
    "\n",
    "### Overfitting\n",
    "Overfitting occurs when a model performs exceptionally well on the training data but fails to generalize to new, unseen data. In the context of deep neural networks, overfitting can happen when the model becomes too complex and learns to memorize the training examples, including noise and irrelevant patterns. This complexity arises due to:\n",
    "\n",
    "- Increasing the `number of layers and neurons` in a deep neural network has higher capacity, allowing it to fit the training data closely, including noise and irrelevant patterns.\n",
    "- A low `learning rate` can cause the model to converge quickly but may result in overshooting the optimal weights.\n",
    "- A low `dropout rate` (i.e., keeping most neurons active) can lead to overfitting.\n",
    "- Using a small `batch size` can introduce noise and make the training process more erratic, potentially leading to overfitting. Larger batch sizes provide more stable updates and can help generalize better.\n",
    "\n",
    "Solutions to address overfitting in deep neural networks:\n",
    "\n",
    "- **Increase training data:** Having more diverse and representative training data can help the model learn better and reduce overfitting.\n",
    "- **Regularization techniques:** Techniques like L1/L2 regularization or dropout can be applied to penalize large weights and encourage model generalization.\n",
    "- **Early stopping:** Stop training the model when the performance on a validation set starts to deteriorate, preventing it from over-optimizing on the training data.\n",
    "- **Model architecture adjustments:** Reduce model complexity by reducing the number of layers or neurons to prevent overfitting.\n",
    "- **Data augmentation:** Apply techniques like rotation, translation, or scaling to artificially increase the diversity of the training data.\n",
    "- **Ensemble learning:** Combine predictions from multiple models to reduce overfitting and improve generalization.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "Underfitting occurs when a model is too simplistic and fails to capture the underlying patterns in the training data. It results in poor performance on both the training and unseen data. Underfitting can be a result of any of the following cases:\n",
    "\n",
    "- If the model is too simple, it may lack the capacity to capture complex patterns in the data, resulting in underfitting.\n",
    "- A high `learning rate` may cause the model to converge slowly or get stuck in a suboptimal solution, resulting in underfitting.\n",
    "- High `dropout rate` can excessively deactivate neurons and prevent the model from learning meaningful representations, causing underfitting.\n",
    "- Insufficient data augmentation techniques or using very limited variations in the training data can result in underfitting. Employing more diverse data augmentation strategies can help the model capture a wider range of patterns.\n",
    "\n",
    "Solutions to address underfitting in deep neural networks:\n",
    "- **Increase model complexity:** Add more layers, increase the number of neurons, or explore more advanced architectures to allow the model to capture complex patterns.\n",
    "- **Feature engineering:** Extract and include relevant features that can help the model better represent the data.\n",
    "- **Reduce regularization:** If the model is underfitting due to excessive regularization, reducing the strength of regularization techniques can help improve performance.\n",
    "- **Increase training time:** Allow the model to train for more epochs or iterations to learn better representations and improve performance.\n",
    "- **Adjust hyperparameters:** Experiment with different learning rates, batch sizes, or optimization algorithms to find better settings for the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
