{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propogation\n",
    "\n",
    "Backpropagation is a widely used algorithm for training neural networks. It enables the computation of gradients with respect to the network's parameters, such as weights and biases, by propagating the error (difference between predicted and actual output) backward through the network. These gradients are then used to update the parameters using gradient descent optimization.\n",
    "\n",
    "The backpropagation algorithm consists of the following steps:\n",
    "\n",
    "- **Forward Propagation:** Compute the outputs of each layer in the network using the current weights and biases.\n",
    "- **Loss Calculation:** Compute the loss between the predicted output and the true output using the chosen loss function.\n",
    "- **Backward Pass and Gradient Calculation:** Propagate the error backward through the network to compute the gradients of the weights and biases.\n",
    "- **Parameter Updates:** Update the weights and biases by subtracting the gradients multiplied by the learning rate. This adjusts the parameters to minimize the loss function.\n",
    "- **Repeat:** Iterate over steps 1-4 until convergence or a maximum number of iterations.\n",
    "\n",
    "## Loss Functions\n",
    "Before diving into backpropagation, let's discuss the choice of loss functions. The loss function measures the discrepancy between the predicted output of the neural network and the true output. Common loss functions include:\n",
    "\n",
    "1. **Mean Squared Error (MSE):** MSE is typically used for regression problems.\n",
    "\n",
    "    $MSE = \\frac{1}{N} * \\sum(y_{pred} - y_{true})^2$\n",
    "\n",
    "    where `y_pred` is the predicted output, `y_true` is the true output, and `N` is the number of samples.\n",
    "\n",
    "2. **Cross-Entropy Loss:** Cross-entropy loss is commonly used for classification problems.\n",
    "\n",
    "    $CrossEntropy = -\\frac{1}{N} * \\sum y_{true} * log(y_{pred})$\n",
    "\n",
    "    where `y_pred` is the predicted output (probability distribution), `y_true` is the true output (one-hot encoded), and `N` is the number of samples.\n",
    "\n",
    "## Backward Pass and Gradient Calculation\n",
    "\n",
    "During the backward pass, the gradients of the loss with respect to the parameters are computed using the chain rule. Here are the equations for computing the gradients in a standard feedforward neural network with a single hidden layer:\n",
    "\n",
    "![Back Propogation](./../../assets/backprop.jpg)\n",
    "\n",
    "### Notation\n",
    "\n",
    "$x_{ij} => \\begin{cases}\n",
    "    i: i^{th}\\ data\\ point \\\\\n",
    "    j: index\\ of\\ dimension/feature\n",
    "\\end{cases}$\n",
    "\n",
    "$f_{ij}/O_{ij} => \\begin{cases}\n",
    "    i: Current\\ Layer\\ Index \\\\\n",
    "    j: Corresponding\\ Neuron\\ Index\\ for\\ that\\ Layer\n",
    "\\end{cases}$\n",
    "\n",
    "$w_{ij}^k => \\begin{cases}\n",
    "    i: From (Corresponding\\ Neuron\\ Index\\ for\\ Current\\ Layer) \\\\\n",
    "    j: To (Corresponding\\ Neuron\\ Index\\ for\\ Next\\ Layer) \\\\\n",
    "    k: Next\\ Layer\\ Index\n",
    "\\end{cases}$\n",
    "\n",
    "$b_{ij} => \\begin{cases}\n",
    "    i: Next\\ Layer\\ Index \\\\\n",
    "    j: To (Corresponding\\ Neuron\\ Index\\ for\\ Next\\ Layer) \\\\\n",
    "\\end{cases}$\n",
    "\n",
    "$\n",
    "w^1 = \\begin{bmatrix}\n",
    "w_{11}^1 & w_{12}^1 \\\\\n",
    "w_{21}^1 & w_{22}^1 \\\\\n",
    "w_{31}^1 & w_{32}^1 \\\\\n",
    "\\end{bmatrix}_{3X2} \\quad\\quad\\quad\\quad\\quad\n",
    "w^2 = \\begin{bmatrix}\n",
    "w_{11}^2 \\\\\n",
    "w_{21}^2\n",
    "\\end{bmatrix}_{2X1}\n",
    "$\n",
    "\n",
    "$\n",
    "b^1 = \\begin{bmatrix}\n",
    "b_{11} & b_{12}\n",
    "\\end{bmatrix}_{1X2} \\quad\\quad\\quad\\quad\\quad\n",
    "b^2 = \\begin{bmatrix}\n",
    "b_{21}\n",
    "\\end{bmatrix}_{1X1}\n",
    "$\n",
    "\n",
    "$Shape\\ of\\ w^i = (Number\\ of\\ Neurons\\ in\\ Currect\\ Layer)\\ X\\ (Number\\ of\\ Neurons\\ in\\ Next\\ Layer)$\n",
    "\n",
    "$Shape\\ of\\ b^i = 1\\ X\\ (Number\\ of\\ Neurons\\ in\\ Next\\ Layer)$\n",
    "\n",
    "### Gradients for Output Layer (w.r.t $w^2$)\n",
    "\n",
    "In the context of a neural network, the gradient represents the slope or direction of steepest ascent of a loss function with respect to the model's parameters (weights and biases). By following the opposite direction of the gradient, we can update the parameters to minimize the loss function and improve the model's performance.\n",
    "\n",
    "$\n",
    "\\frac{∂L}{∂w_{11}^2} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂w_{11}^2} \\quad\\quad\\quad\\quad\\quad\n",
    "\\frac{∂L}{∂w_{21}^2} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂w_{21}^2} \\\\\n",
    "\\frac{∂L}{∂b_2} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂b_2}\n",
    "$\n",
    "\n",
    "This can be represented in equation as:\n",
    "\n",
    "```scss\n",
    "delta_output = (y_pred - y_true) * activation_derivative(output)\n",
    "dW_output = 1/N * (hidden_output.T @ delta_output)\n",
    "db_output = 1/N * sum(delta_output)\n",
    "```\n",
    "\n",
    "where `y_pred` is the predicted output, `y_true` is the true output, `activation_derivative` is the derivative of the activation function applied to the output, `hidden_output` is the output of the hidden layer, `delta_output` is the error term for the output layer, `dW_output` is the gradient of the weights connecting the hidden layer to the output layer, and `db_output` is the gradient of the biases in the output layer.\n",
    "\n",
    "### Gradients for Hidden Layer (w.r.t $w^1$)\n",
    "\n",
    "$\n",
    "\\frac{∂L}{∂w_{11}^1} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂O_{11}} * \\frac{∂O_{11}}{∂w_{11}^1} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\n",
    "\\frac{∂L}{∂w_{12}^2} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂O_{12}^2} * \\frac{∂O_{12}}{∂w_{12}^1} \\\\\n",
    "\\frac{∂L}{∂w_{21}^1} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂O_{11}} * \\frac{∂O_{11}}{∂w_{21}^1} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\n",
    "\\frac{∂L}{∂w_{22}^2} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂O_{12}^2} * \\frac{∂O_{12}}{∂w_{22}^1} \\\\\n",
    "\\frac{∂L}{∂w_{31}^1} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂O_{11}} * \\frac{∂O_{11}}{∂w_{31}^1} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\n",
    "\\frac{∂L}{∂w_{32}^2} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂O_{12}^2} * \\frac{∂O_{12}}{∂w_{32}^1} \\\\\n",
    "\\frac{∂L}{∂b_{11}} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂O_{11}} * \\frac{∂O_{11}}{∂b_{11}} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\n",
    "\\frac{∂L}{∂b_{12}} = \\frac{∂L}{∂O_{21}} * \\frac{∂O_{21}}{∂O_{12}} * \\frac{∂O_{12}}{∂b_{12}}\n",
    "$\n",
    "\n",
    "This can be represented in equation as:\n",
    "\n",
    "```scss\n",
    "delta_hidden = (delta_output @ W_output.T) * activation_derivative(hidden_output)\n",
    "dW_hidden = 1/N * (input.T @ delta_hidden)\n",
    "db_hidden = 1/N * sum(delta_hidden)\n",
    "```\n",
    "\n",
    "where `W_output` is the weight matrix connecting the hidden layer to the output layer, `delta_hidden` is the error term for the hidden layer, `dW_hidden` is the gradient of the weights connecting the input layer to the hidden layer, and `db_hidden` is the gradient of the biases in the hidden layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Updates\n",
    "After calculating the gradients, the parameters (weights and biases) are updated using gradient descent to minimize the loss function. The update equations for a parameter p are:\n",
    "\n",
    "$\n",
    "P_{updated} = P_{old} - learning\\_rate * dP\n",
    "$\n",
    "\n",
    "where `learning_rate` is a hyperparameter that determines the step size for the parameter update, and `dp` is the gradient of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights with random values\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.hidden_layer = self.sigmoid(np.dot(X, self.W1))\n",
    "        self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.W2))\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        output_error = y - self.output_layer\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output_layer)\n",
    "        hidden_error = output_delta.dot(self.W2.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_layer)\n",
    "\n",
    "        # Update the weights\n",
    "        self.W2 -= self.hidden_layer.T.dot(output_delta) * learning_rate\n",
    "        self.W1 -= X.T.dot(hidden_delta) * learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return self.output_layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we implement a simple MLP with one hidden layer and a sigmoid activation function. The `forward` method performs the forward pass, and the `backward` method computes the gradients and updates the parameters using the backpropagation algorithm. The `train` method iterates over the training data and performs multiple iterations of the backward pass to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08292542]\n",
      " [0.92638874]\n",
      " [0.92569163]\n",
      " [0.05634042]]\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Create an MLP object\n",
    "mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# Train the MLP\n",
    "mlp.train(X, y, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "# Test the MLP\n",
    "predictions = mlp.predict(X)\n",
    "print(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create an MLP with 2 input neurons, 4 hidden neurons, and 1 output neuron. We train the network on the `XOR` problem using the input data `X` and target outputs `y` for 1000 epochs with a learning rate of 0.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
