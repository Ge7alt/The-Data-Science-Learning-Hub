{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "The very first and the simplest neural network model was known as Perceptron, which was designed by Rosenblatt in 1977. Perceptron was inspired by the human nervous system. The working mechanism of perceptron is very similar to that of logistic regression.\n",
    "\n",
    "## Logistic Regression vs Perceptron\n",
    "\n",
    "Logistic regression is a binary classification algorithm that uses a linear function followed by a logistic/sigmoid activation function to estimate the probability of belonging to a particular class.\n",
    "\n",
    "A perceptron is a basic building block of a neural network. It takes a set of input features, applies weights to them, and produces an output using a step function.\n",
    "\n",
    "![Perceptron vs Logistic Regression](./../../assets/perceptron.jpg)\n",
    "\n",
    "The above diagram is the generalization of both perceptron and logistic regression together. In both the cases (Perceptron and Logistic Regression) the weighted sum of the input features, along with the bias term, is passed through a activation function to produce the predicted output. The only difference between Perceptron and Logistic Regression is the activation function.\n",
    "\n",
    "For logistic regression:\n",
    "    $f(z_i) = \\frac{1}{1 + e^{-z_i}}$\n",
    "\n",
    "For Perceptron:\n",
    "$f(z_i) = \\begin{cases}\n",
    "    1, & \\text{if } z_i > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        self.activation_function = lambda x: 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        linear_output = np.dot(self.weights, x) + self.bias\n",
    "        return self.activation_function(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron prediction: 1\n"
     ]
    }
   ],
   "source": [
    "# Create a perceptron with 2 input features and using the step function as the activation function\n",
    "perceptron = Perceptron(2)\n",
    "\n",
    "# Set the weights and bias\n",
    "perceptron.weights = np.array([0.5, -0.5])\n",
    "perceptron.bias = 0.2\n",
    "\n",
    "# Make predictions\n",
    "x = np.array([0.3, -0.7])\n",
    "prediction = perceptron.predict(x)\n",
    "print(\"Perceptron prediction:\", prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "MLP (aka feedforward neural network) is an extension of the perceptron, where multiple layers of neurons are stacked together. It consists of an input layer, one or more hidden layers, and an output layer. Each neuron in a layer is connected to all neurons in the adjacent layers. The hidden layers enable the model to learn more complex representations of the input data.\n",
    "\n",
    "The architecture of MLP can be represented as follows:\n",
    "\n",
    "![Multi-Layer Perceptron](./../../assets/mlp.png)\n",
    "\n",
    "```scss\n",
    "Input Layer -> Receives the input features and passes them to the hidden layers\n",
    "Hidden Layer(s) -> Applies a weighted sum of inputs and an activation function to produce an output\n",
    "Output Layer -> Applies the same process to generate the final output\n",
    "```\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "An activation function introduces non-linearity into the network, enabling it to learn complex relationships between inputs and outputs. Commonly used activation functions include:\n",
    "\n",
    "**Sigmoid:** It squashes the weighted sum into the range [0, 1].\n",
    "\n",
    "$Sigmoid(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "**Tanh:** Similar to sigmoid, but squashes the weighted sum into the range [-1, 1].\n",
    "\n",
    "$Tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "**ReLU (Rectified Linear Unit):** Sets negative values to 0 and keeps positive values as they are.\n",
    "\n",
    "$ReLU(x) = max(0, x)$\n",
    "\n",
    "**Leaky ReLU:** Similar to ReLU, but allows a small negative value for negative inputs.\n",
    "\n",
    "$Leaky-ReLU(x) = max(0.01*x, x)$\n",
    "\n",
    "Here are few more activation functions used around, with their corresponding graphs:\n",
    "\n",
    "![Activation Functions](./../../assets/activation.jpeg)\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "Forward propagation is the process of computing the output of an MLP given an input. Each neuron takes the weighted sum of its inputs, adds a bias term, and applies an activation function to produce the output.\n",
    "\n",
    "The forward propagation equations for an MLP with one hidden layer can be represented as follows:\n",
    "\n",
    "```scss\n",
    "z1 = activation_function(W1 * x + b1)\n",
    "z2 = activation_function(W2 * z1 + b2)\n",
    "...\n",
    "output = activation_function(W_output * z_last_hidden + b_output)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- `x` is the input vector,\n",
    "- `W1, W2, ..., W_output` are the weight matrices for each layer,\n",
    "- `b1, b2, ..., b_output` are the bias vectors for each layer,\n",
    "- `z1, z2, ..., z_last_hidden` are the outputs of each hidden layer, and\n",
    "- `output` is the final output of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation='sigmoid'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            weight_matrix = np.random.randn(layer_sizes[i], layer_sizes[i+1])\n",
    "            self.weights.append(weight_matrix)\n",
    "            bias_vector = np.zeros(layer_sizes[i+1])\n",
    "            self.biases.append(bias_vector)\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation_function = self._sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_function = self._tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation_function = self._relu\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            linear_output = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            output = self.activation_function(linear_output)\n",
    "            activations.append(output)\n",
    "\n",
    "        return activations[-1]\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation includes the architecture, forward propagation, and activation functions for the MLP. It supports different activation functions such as sigmoid, tanh, and ReLU.\n",
    "\n",
    "You can then create an instance of the MLP class and use it for various tasks, including classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54564303 0.67919588 0.37609245]\n",
      " [0.54153638 0.72084591 0.20390913]\n",
      " [0.52509919 0.71018972 0.2952373 ]\n",
      " [0.53787453 0.68127392 0.34917849]\n",
      " [0.58678118 0.63960352 0.29108797]\n",
      " [0.5503897  0.65551025 0.3184623 ]\n",
      " [0.67536001 0.68319327 0.12238282]\n",
      " [0.71297346 0.63467466 0.12780863]\n",
      " [0.59128551 0.68059875 0.2756035 ]\n",
      " [0.64998348 0.71544514 0.10785669]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "mlp = MLP(input_size=4, hidden_sizes=[8, 6], output_size=3, activation='sigmoid')\n",
    "input_data = np.random.randn(10, 4)  # Example input data with 10 samples\n",
    "output = mlp.forward(input_data)  # Compute the MLP's output\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create an MLP with an input size of 4, two hidden layers of sizes 8 and 6, and an output size of 3. We then generate random input data and compute the MLP's output using the `forward` method.\n",
    "\n",
    "## References\n",
    "\n",
    "- [MultilayerPerceptron: A simple multilayer neural network](https://rasbt.github.io/mlxtend/user_guide/classifier/MultiLayerPerceptron/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
