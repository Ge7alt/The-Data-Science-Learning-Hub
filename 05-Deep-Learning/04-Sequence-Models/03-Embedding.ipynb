{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "In the context of natural language processing (NLP), \"embeddings\" refer to dense vector representations of words (or sometimes phrases and sentences) in a continuous vector space. These vector representations are learned through unsupervised machine learning techniques like Word2Vec, GloVe, or FastText, where words with similar meanings or appearing in similar contexts are mapped to vectors that are close together in the vector space.\n",
    "\n",
    "In traditional NLP, words have been typically represented using one-hot encoding, where each word is represented as a sparse binary vector, with a 1 in the position corresponding to the word's index in the vocabulary and 0s everywhere else. However, one-hot encoded vectors suffer from several limitations:\n",
    "\n",
    "- **High Dimensionality:** One-hot encoded vectors are very high-dimensional, with the dimensionality equal to the size of the vocabulary. This leads to increased computational complexity and storage requirements.\n",
    "\n",
    "- **Lack of Semantic Information:** One-hot vectors do not capture any semantic relationships between words. Each word is treated as an isolated entity with no notion of similarity or relatedness to other words.\n",
    "\n",
    "Embeddings address these limitations and offer several advantages in NLP:\n",
    "\n",
    "- **Low-dimensional Dense Representations:** Word embeddings are low-dimensional dense vectors, typically ranging from 50 to 300 dimensions, making them computationally efficient and memory-friendly compared to one-hot vectors.\n",
    "- **Semantic Relationships:** Embeddings capture semantic relationships between words. Words with similar meanings or appearing in similar contexts will have similar vector representations, enabling models to understand the meaning and context of words.\n",
    "- **Generalization:** Word embeddings allow NLP models to generalize better across different tasks and datasets. Pre-trained word embeddings can be used as features for various downstream tasks, even if the training data for the downstream task is limited.\n",
    "- **Out-of-Vocabulary (OOV) Words:** Word embeddings provide representations for words not seen during training (OOV words) by generalizing from the context of other words.\n",
    "- **Efficiency:** Once trained, word embeddings can be efficiently stored and reused, which is especially important for large-scale NLP applications.\n",
    "- **Capturing Analogies:** Word embeddings can capture analogical relationships like \"king\" - \"man\" + \"woman\" ≈ \"queen,\" allowing models to perform analogy-based reasoning.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Word2Vec is a popular technique for learning word embeddings, which are dense vector representations of words in a continuous vector space. Word embeddings capture semantic relationships between words, allowing machines to understand and work with words in a more meaningful way. Word2Vec was introduced by researchers at Google in 2013, and it has since become one of the foundational techniques in natural language processing (NLP) and other related fields.\n",
    "\n",
    "The basic idea behind Word2Vec is to represent each word in a high-dimensional vector space, where words with similar meanings or contexts are located close to each other. The key intuition behind Word2Vec is the distributional hypothesis, which posits that words appearing in similar contexts tend to have similar meanings. For example, in the sentences \"I love cats\" and \"I adore felines,\" the words \"love\" and \"adore\" are likely to be used in similar contexts and have similar semantic meanings.\n",
    "\n",
    "Word2Vec can be trained using two main architectures: Continuous Bag of Words (CBOW) and Skip-gram. Let's explore each of these in detail:\n",
    "\n",
    "### 1. Continuous Bag of Words (CBOW)\n",
    "CBOW aims to predict a target word based on its surrounding context words. Given a sequence of words in a sentence, CBOW tries to predict the middle word based on the surrounding context words. The context window size determines how many words before and after the target word are considered as the context.\n",
    "\n",
    "For example, consider the sentence: \"The cat sat on the mat.\" If we set the context window size to 2 and assume \"sat\" is the target word, CBOW will use the context words \"The,\" \"cat,\" \"on,\" and \"the\" to predict the word \"sat.\"\n",
    "\n",
    "The architecture involves the following steps:\n",
    "- Convert the context words to their corresponding word embeddings.\n",
    "- Average these embeddings to create a context vector.\n",
    "- Use this context vector as input to a neural network to predict the target word.\n",
    "\n",
    "Let's implement it using python. Python provides a package named `gensim` to make our job easy. Let's begin by installing the package.\n",
    "\n",
    "```\n",
    "pip install gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus (list of sentences)\n",
    "corpus = [\n",
    "    \"I love cats\",\n",
    "    \"I adore felines\",\n",
    "    \"Dogs are loyal\",\n",
    "    \"Cats and dogs are pets\",\n",
    "    \"The sun is shining\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
    "\n",
    "# CBOW model\n",
    "cbow_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=2, sg=0, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find similar words using the trained model\n",
    "def find_similar_words(model, word, top_n=5):\n",
    "    similar_words = model.wv.most_similar(positive=[word], topn=top_n)\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW - Similar words to 'cats': [('pets', 0.19913175702095032), ('felines', 0.17272792756557465), ('shining', 0.17018885910511017), ('sun', 0.14589877426624298), ('is', 0.06408977508544922)]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "target_word = \"cats\"\n",
    "print(f\"CBOW - Similar words to '{target_word}': {find_similar_words(cbow_model, target_word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Skip-gram\n",
    "Skip-gram works in the opposite way of CBOW. It aims to predict context words given a target word. In other words, it tries to find the context words that are most likely to appear in the given sentence with a particular target word.\n",
    "\n",
    "For the same example sentence, \"The cat sat on the mat,\" if \"sat\" is the target word, Skip-gram will try to predict the context words \"The,\" \"cat,\" \"on,\" and \"the.\"\n",
    "\n",
    "The architecture involves the following steps:\n",
    "- Convert the target word to its corresponding word embedding.\n",
    "- Use this embedding as input to a neural network to predict the context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-gram model\n",
    "skipgram_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=2, sg=1, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram - Similar words to 'cats': [('pets', 0.19913214445114136), ('felines', 0.17272792756557465), ('shining', 0.17018885910511017), ('sun', 0.1459500789642334), ('is', 0.06408977508544922)]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "target_word = \"cats\"\n",
    "print(f\"Skip-gram - Similar words to '{target_word}': {find_similar_words(skipgram_model, target_word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is trained using a large corpus of text data, and the neural network parameters (word embeddings) are adjusted during training to maximize the likelihood of correctly predicting the context words or target words. Once trained, the word embeddings can be used in various downstream NLP tasks, such as sentiment analysis, machine translation, and named entity recognition, among others.\n",
    "\n",
    "The resulting word embeddings capture semantic relationships between words. Words with similar meanings or appearing in similar contexts will have similar vector representations, and their Euclidean or cosine distances in the vector space will be small. This property allows the word embeddings to be used in similarity calculations, analogy tasks (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\"), and even for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Word Embedding\n",
    "GloVe stands for Global Vectors for word representation and was developed by researchers at Stanford University. It is unsupervised learning algorithm aiming to generate word embeddings by aggregating global word co-occurrence matrices from a given corpus. To start with GloVe, first we have to download the pre-trained model hosted [here](https://nlp.stanford.edu/projects/glove/). A total of four pre-trained models are available there. Get your own choice.\n",
    "\n",
    "The basic idea behind the GloVe word embedding is to derive the relationship between the words from statistics.\n",
    "\n",
    "To use glove word embedding with our way, you first need to install python scipy and numpy libraries (if not installed already). Copy the below command to do so.\n",
    "\n",
    "```\n",
    "pip3 install scipy\n",
    "pip3 install numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "glove_filepath = 'glove.6B.50d.txt'\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(glove_filepath, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "\n",
    "# Find similar Vectors\n",
    "def find_similar_vectors(inputs):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embeddings_dict[inputs]))\n",
    "\n",
    "# Example:\n",
    "find_similar_vectors('king')[:5]    # Get top 5 similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Word Embedding\n",
    "\n",
    "Facebook AI Research lab developed an open-source word-embedding library called `FastText` with the purpose of achieving more accurate and scalable solutions qucikly while processing large text data. Similar to `GloVe` Word Embedding, `FastText` is also the modified version of `Word2Vec`.\n",
    "\n",
    "Unlike Word2Vec which feeds individual words to neural network, FastText breaks a word into character n-grams and then feeds those character n-grams to the neural network. For instance: the tri-gram of the word fasttext is:\n",
    "\n",
    "`<fa`, `fas`, `ast`, `stt`, `tte`, `tex`, `ext`, `xt>`\n",
    "\n",
    "The embedding vectors for each of these words are obtained after training the neural network. These independent embedding vectors are finally added up to obtain the word embedding vector of the original word `fasttext`.\n",
    "\n",
    "**How is FastText better than Word2Vec?**\n",
    "\n",
    "- Compound words like `fasttext` can be properly represented even if the data do not contain the word `fasttext` as other words like `fast` and `text` contain the same n-grams.\n",
    "- Even though the words like `fast`, `faster`, `fastest` share the same redical, word2vec handles them independently according to the context. FastText on the other hand facilitates parameter sharing among such words and does efficient utilization of the morphological structure.\n",
    "\n",
    "Let's try implementing it for real. Python provides an open-source library `gensim` that makes working with fasttext easy. Let's begin by installing gensim library. We will use `nltk` for preprocessing, so let's install both of the libraries.\n",
    "\n",
    "```\n",
    "pip3 install nltk\n",
    "pip3 install gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"This is a sentence.\", \"This is another sentence.\"]\n",
    "data = [word_tokenize(sentence) for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(data, vector_size=128, window=5, min_count=1, workers=4,sg=1)\n",
    "# model.save('./fasttext.ft')\n",
    "fmodel = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 0.25602468848228455),\n",
       " ('is', 0.1581001877784729),\n",
       " ('.', 0.05240068957209587)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fmodel['this']\n",
    "fmodel.similar_by_word('this', topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will first load the text data into a list of strings. Then, it will create a FastText model with a vector size of 128, a window size of 5, and a minimum count of 1. The model will then be trained on the text data. Finally, the word embeddings for the word \"this\" will be printed.\n",
    "\n",
    "Here is an explanation of the code:\n",
    "\n",
    "- The `gensim.models.FastText` class is used to create a FastText model.\n",
    "- The `vector_size` parameter specifies the size of the word embeddings.\n",
    "- The `window` parameter specifies the size of the context window.\n",
    "- The `min_count` parameter specifies the minimum number of times a word must appear in the text data in order to be included in the model.\n",
    "- The `model.wv` property returns KeyedVectors object that contains the word embeddings. The KeyedVectors object has a number of methods that can be used to access and manipulate the word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a neural network model that was pre-trained on a massive dataset of text and code. It can be used for a variety of natural language processing (NLP) tasks, such as question answering, text classification, and sentiment analysis.\n",
    "\n",
    "**How does BERT work?**\n",
    "\n",
    "BERT is a transformer-based model, which means that it uses a stack of self-attention layers to learn the relationships between words in a sentence. The model is pre-trained on a massive dataset of text and code, which allows it to learn the contextual meaning of words.\n",
    "\n",
    "**How to use BERT for embedding?**\n",
    "\n",
    "BERT can be used to generate word embeddings, which are vector representations of words that capture their semantic meaning. To generate word embeddings using BERT, you first need to tokenize the input text into individual words or subwords (using the BERT tokenizer). You can then pass the tokenized input through the BERT model to generate a sequence of hidden states. The hidden states can then be used to represent the words in the input text.\n",
    "\n",
    "To implement BERT, we will use HuggingFace's `transformers` library and `transformers` requires `pytorch` installed. So let's begin by installing the required libraries.\n",
    "\n",
    "```\n",
    "pip3 install torch\n",
    "pip3 install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
      "         1012,   102]) torch.Size([12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the input text\n",
    "text = 'The quick brown fox jumps over the lazy dog.'\n",
    "tokens = tokenizer(text=text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the BERT embeddings\n",
    "embeddings = model(**tokens).last_hidden_state\n",
    "\n",
    "# Print the embeddings for each token\n",
    "for token, embedding in zip(tokens[\"input_ids\"], embeddings):\n",
    "    print(token, embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will first load the BERT model and the BERT tokenizer. It will then tokenize the input text and convert the tokens to IDs. The IDs will then be passed to the BERT model, which will generate a sequence of hidden states. The hidden states will then be used to represent the words in the input text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
