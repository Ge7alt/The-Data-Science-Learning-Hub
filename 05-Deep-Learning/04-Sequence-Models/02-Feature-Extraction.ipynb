{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction from Text Data\n",
    "\n",
    "Machine learning algorithms cannot work with the raw text data. So it must, first, be converted into binary codes of 1s and 0s. This is process is popularly known as feature extraction of text data.\n",
    "\n",
    "Some widely used methods for feature extraction of text data are listed below: \n",
    "\n",
    "![Embedding Classification](./../../assets/embedding-classification.jpg)\n",
    "\n",
    "Here, we will be discussing only about Traditional approaches of word embedding. The remaining two will be discussed separately as different notebook.\n",
    "\n",
    "## Bag of words (BoW)\n",
    "\n",
    "**Bag of Words** model is the simplest and most popular form of word embedding. The key idea of **BoW** models is to encode every word in the vocabulary as one-hot-encoded vector.\n",
    "\n",
    "If r1, r2 and r3 be three records, the vectors corresponding to r1, r2 and r3 be v1, v2 and v3 respectively such that r1 and r2 are more similar to each other as compared to r3. Then, as general understanding, the vector distance between v1 and v2 is less than that between v1 and v3 or v2 and v3.\n",
    "\n",
    "<p align=\"center\"><b>\n",
    "    distance (v1, v2) < distance (v1, v3)<br/>\n",
    "    similarity (r1, r2) > similarity (r1, r3)\n",
    "</b></p>\n",
    "\n",
    "For easy understanding, let us consider a sweet example. Let there be three reviews for a product in ecommerce site as:\n",
    "\n",
    "    r1: This product is good and is affordable.\n",
    "    r2: This product is not good and affordable.\n",
    "    r3: This product is good and cheap.\n",
    "\n",
    "Let's see how BoW encodes the text data to machine compatible form. Follow along with the below points:\n",
    "\n",
    "**I. Construct a set of all the unique words present in the corpus:**\n",
    "\n",
    "    { this, product, is, good, and, affordable, not, cheap }\n",
    "\n",
    "There are a total of 8 uique words in the set formed. So the size of the vector generated for each review will be 8 as well, with the index position starting from 0 and ending to 7 i.e. \n",
    "\n",
    "    { 0: this, 1: product, 2: is, 3: good, 4: and, 5: affordable, 6: not, 7: cheap }\n",
    "\n",
    "**II. Construct a d-dimensional vector for each review separately:**\n",
    "\n",
    "Construct a d-dimensional vector (*d* being the vocabulary size) for each review. Each index/dimension of the vector corresponds to a unique word in the vocabulary. The value in each cell of the vector represents the number of times the word with that index occurs in the corpus.\n",
    "\n",
    " d | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "---|---|---|---|---|---|---|---|---|\n",
    "**v1**| 1 | 1 | 2 | 1 | 1 | 1 | 0 | 0 |\n",
    "**v2**| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 |\n",
    "**v3**| 1 | 1 | 1 | 1 | 1 | 0 | 0 | 1 |\n",
    "\n",
    "<p style=\"text-align:center;\"><i><b>Table :</b> 8 dimensional vector representation of each review</i></p>\n",
    "\n",
    "### Objective\n",
    "\n",
    "Similar texts (reviews, in this case) must result closer vector.\n",
    "\n",
    "    distance(v1-v2) = √((1-1)²+(1-1)²+(2-1)²+(1-1)²+(1-1)²+(1-1)²+(0-1)²+(0-0)²) = √2\n",
    "    distance(v1-v3) = √((1-1)²+(1-1)²+(2-1)²+(1-1)²+(1-1)²+(1-0)²+(0-0)²+(0-1)²) = √3 \n",
    "\n",
    "The Euclidean distance between vectors v1 and v2 is less than that between v1 and v3. However the meaning of review r1 is completely opposite to that of review r2. Thus, BoW does not preserve the semantic meaning of a words and fails to work when there is small change in the text statements.\n",
    "\n",
    "> There is another idea of BoW where we use booleans *0* and *1* instead of number of occurance of the word in the corpus, known as binary BoW. If the word exists at least once the corresponding cell is assigned a binary value 1, otherwise 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This product is good and is affordable.',\n",
    "    'This product is not good and affordable.',\n",
    "    'This product is good and cheap.'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "output = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "- Vector length is insanely large for large corpus.\n",
    "- BoW results to sparse matrix, which is what we would like to avoid.\n",
    "- Retains no information about grammar and ordering of words in a corpus.\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "In NLP an independent text entity is known as document and the collection of all these documents over the project space is known as corpus. *tf-idf* stands for Term Frequency-Inverse Document Frequency. The entire technique can be studied by studying *tf* and *idf* separately.\n",
    "\n",
    "**Term-Frequency** is a measure of frequency of appearance of term *t* in a document *d*. In other words, the probability of finding term *t* in a document *d*. \n",
    "\n",
    "<p align=\"center\">\n",
    "    {% mathjax %}\n",
    "        tf_{t,d} = \\frac{No \\hspace{1mm} of \\hspace{1mm} times \\hspace{1mm} t \\hspace{1mm} appears \\hspace{1mm} in \\hspace{1mm} d}{Total \\hspace{1mm} no \\hspace{1mm} of \\hspace{1mm} terms \\hspace{1mm} in \\hspace{1mm} d}\n",
    "    {% endmathjax %}\n",
    "</p>\n",
    "\n",
    "**Inverse-Document-Frequency** is a measure of inverse of probability of finding a document that contains term t in a corpus. In other words, a measure of the importance of term t.\n",
    "\n",
    "<p align=\"center\">\n",
    "    {% mathjax %}\n",
    "        idf_{t} = log \\hspace{1mm} \\frac{Total \\hspace{1mm} no \\hspace{1mm} of \\hspace{1mm} documents \\hspace{1mm} in \\hspace{1mm} corpus}{No \\hspace{1mm} of \\hspace{1mm} documents \\hspace{1mm} with \\hspace{1mm} term \\hspace{1mm} t}\n",
    "    {% endmathjax %}\n",
    "</p>\n",
    "\n",
    "We can now compute the *tf-idf* score for each word in the corpus. *tf-idf* gives us the similarity between two documents in the corpus. Words with a higher score are more important. *tf-idf* score is high when both *idf* and *tf* values are high. So, *tf-idf* gives more importance to words that are:\n",
    "\n",
    "- More frequent in the entire corpus\n",
    "- Rare in the corpus but frequent in the document.\n",
    "\n",
    "Now this *tf-idf* score is used as a value for each cell of the document-term matrix, just like the frequency of words in case of Bag-of-Words. The formula below is used to compute *tf-idf* score for each cell:\n",
    "\n",
    "<p align=\"center\">\n",
    "    {% mathjax %}\n",
    "        (tf-idf)_{t,d} = tf_{t,d} * idf_{t}\n",
    "    {% endmathjax %}\n",
    "</p>\n",
    "\n",
    "While computing *tf*, all terms are considered equally important. However, it is known that certain terms, such as *is*, *of*, *and*, *that*, *the*, etc may appear a lot of times but have no or little importance. Thus we need to weigh down such frequent terms while scaling the rare ones up using *idf*.\n",
    "\n",
    " Term | tf (r1) | tf (r2) | tf (r3)| idf | tf-idf (r1) | tf-idf (r2) | tf-idf (r3)\n",
    "---|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "this| 1/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "product| 1/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "is| 2/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "good| 1/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "and| 1/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "affordable| 1/7 | 1/7 | 0 | 0.176 | 0.025 | 0.025 | 0.000 |\n",
    "not| 0 | 1/7 | 0 | 0.477 | 0.000 | 0.068 | 0.000 |\n",
    "cheap| 0 | 0 | 1/7 | 0.477 | 0.000 | 0.000 | 0.068 |\n",
    "\n",
    "### Why *log* in *idf*?\n",
    "\n",
    "- If we look into the above formula without *log* function, we notice the value of *tf* is small where as *idf* has much larger value. If we do not use *log*, *idf* dominates and any value of *tf* won't matter.\n",
    "- Log of 1 is 0. Hence when term t is contained in all documents, idf will be zero i.e. inverse similarity is zero and the documents are completely similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This product is good and is affordable.',\n",
    "    'This product is not good and affordable.',\n",
    "    'This product is good and cheap.'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "output = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "\n",
    "- *tf-idf* is based on BoW model. So, it does not capture position, sementics, co-occurance in different documents\n",
    "- Only useful as a lexical level feature\n",
    "\n",
    "\n",
    "## Co-Occurance Matrix\n",
    "\n",
    "*Co-Occurance Matrix* is the representation of corpus in matrix form. It educates us with the information whether two entities co-occur and if so, how frequently. The key components of co-occurance matrix are Focus Word and window length.\n",
    "\n",
    "### Focus word & Window length\n",
    "\n",
    "The word that is currently under study is the focus word and the number of words we are considering around the focus word is the window length. These are the context words. The words representing rows of the table are the focus words and those representing columns are the context words. For this article we are considering window length to be *2*.\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Identify the number of context words for each focus word in the corpus for the given window length\n",
    "\n",
    "**Co-occurance matrix** preserves the semantic relationship between words. It can be used anytime once computed.\n",
    "\n",
    "*↓F - C→*|this|product|is|good|and|affordable|not|cheap\n",
    "---|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:\n",
    "**this**|3|3|3|0|0|0|0|0\n",
    "**product**|3|3|3|2|0|0|1|0\n",
    "**is**|3|3|4|4|3|1|1|0\n",
    "**good**|0|2|4|3|3|1|1|1\n",
    "**and**|0|0|3|3|3|2|1|1\n",
    "**affordable**|0|0|1|1|2|2|0|0\n",
    "**not**|0|1|1|1|1|0|1|0\n",
    "**cheap**|0|0|0|1|1|0|0|1\n",
    "\n",
    "<p align=\"center\"><i>F → Focus Word <b>&</b> C → Context Word</i></p>\n",
    "\n",
    "Let's go through an example to make it more clear.\n",
    "\n",
    "    Focus Word: good\n",
    "    Window Length: 2\n",
    "    Context Words: left → product, is, not\n",
    "                   right → and, is, affordable, cheap\n",
    "\n",
    "Though window length is *2*, why do we see more words on either side of the context? This is because words *{product, is}* lie within 2 context distance on the left of focus word *good* in review *r1* and *r3*. But in case of review *r2*, the context words are *{is, not}*. So the set of context words become *{product, is, not}* on the left side of the focus word. Similarly the set becomes *{and, is, affordable, cheap}* on the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This product is good and is affordable.',\n",
    "    'This product is not good and affordable.',\n",
    "    'This product is good and cheap.'\n",
    "]\n",
    "\n",
    "def clean(sentences):\n",
    "    result = list()\n",
    "    for sentence in sentences:\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        sentence = sentence.translate(table).lower()\n",
    "        sentence = re.sub(' +', ' ', sentence).lstrip().rstrip()\n",
    "        result.append(sentence)\n",
    "    return result\n",
    "\n",
    "def update_matrix(sent, feats, matrix, window_len):\n",
    "    words = sent.split(' ')\n",
    "    for focus_word_idx, focus_word in enumerate(words):    # Iterate each word as focus word\n",
    "        focus_word = focus_word.lower()\n",
    "        x = max(0, focus_word_idx - window_len)\n",
    "        y = min(len(words), focus_word_idx + window_len + 1)\n",
    "        for context_word_idx in range(x, y):\n",
    "            if words[context_word_idx] in feats:\n",
    "                matrix_row_idx = feats.index(focus_word)\n",
    "                matrix_col_idx = feats.index(words[context_word_idx])\n",
    "                matrix[matrix_row_idx][matrix_col_idx] += 1\n",
    "    return matrix\n",
    "\n",
    "corpus = clean(corpus)\n",
    "vectorizer = CountVectorizer(stop_words=None, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "vec = vectorizer.fit_transform(corpus)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "n = len(features)\n",
    "window_len = 2\n",
    "matrix = np.zeros((n, n))   # Initialize co-occurance matrix to 0\n",
    "\n",
    "for sentence in corpus:\n",
    "    result = update_matrix(sentence, features, matrix, window_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `result` variable stores the final co-occurance matrix.\n",
    "\n",
    "### Limitation\n",
    "\n",
    "- Become very complex (insenly large dimension) for large corpus\n",
    "\n",
    "### Solution\n",
    "\n",
    "- Singular value decomposition (SVD) and Principal Component Analysis (PCA) are two eigenvalue methods used to reduce a high-dimensional dataset into fewer dimensions while retaining important information\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "1. [A Survey on Sentence Embedding Models Performance for Patent Analysis](https://arxiv.org/abs/2206.02690)\n",
    "2. [Co-occurrence matrix](https://www.sciencedirect.com/topics/engineering/cooccurrence-matrix)\n",
    "3. [Word Vectors Intuition and Co-Occurrence Matrices](https://towardsdatascience.com/word-vectors-intuition-and-co-occurence-matrixes-a7f67cae16cd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
