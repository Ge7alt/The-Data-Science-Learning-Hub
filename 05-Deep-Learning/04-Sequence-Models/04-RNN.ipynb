{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "Recurrent Neural Network aka RNN is a type of deep learning model specifically designed for processing sequential or time-series data. RNNs are capable of capturing the temporal dependencies and patterns in sequential data by maintaining an internal memory or hidden state that persists as new inputs are processed.\n",
    "\n",
    "The fundamental building block of an RNN is a recurrent neuron, which takes both the current input and the previous hidden state as inputs. It performs computations on these inputs and produces an output and a new hidden state. This hidden state is then fed back into the recurrent neuron for the next time step, creating a recurrent loop that allows the network to remember and learn from previous inputs.\n",
    "\n",
    "The key characteristic of RNNs is that they share the same set of weights and parameters across all time steps, allowing them to handle inputs of variable length. This weight sharing enables the network to learn and generalize patterns across different time steps, capturing long-range dependencies in the sequential data.\n",
    "\n",
    "![Recurrent Neural Network](./../../assets/rnn.png)\n",
    "\n",
    "### Why RNNs?\n",
    "\n",
    "RNNs are required because regular neural networks along with CNNs;\n",
    "\n",
    "- Cannot handle sequential data\n",
    "- Takes into account the current input only\n",
    "\n",
    "### RNNs, they Remember!\n",
    "\n",
    "In traditional neural networks, each input is treated as independent and unrelated to other inputs. However, recurrent neural networks (RNNs) are different. They have the ability to remember and consider the relationships between inputs.\n",
    "\n",
    "Imagine an RNN as a loop that keeps passing information to itself. This loop allows the RNN to maintain an internal memory or hidden state. When the RNN receives an input, it not only processes that input but also takes into account the information it has stored from previous inputs.\n",
    "\n",
    "Think of it like reading a story. In a traditional neural network, you would read each sentence separately, without any context from previous sentences. But in an RNN, you read the sentences in order and remember the information from previous sentences as you move forward. This helps you understand the story better and predict what might come next.\n",
    "\n",
    "The RNN's ability to remember comes from this loop-like structure. It allows the network to capture and store information from previous inputs and use it to influence the processing of future inputs. This memory allows the RNN to recognize patterns, dependencies, and sequences in the data.\n",
    "\n",
    "By remembering and considering the relationships between inputs, RNNs are particularly useful for tasks that involve sequential data, such as predicting the next word in a sentence, generating text, analyzing time series data, and processing speech or natural language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of RNNs\n",
    "\n",
    "Based on the patterns of inputs taken and outputs generated, RNNs can be categorized into five major types:\n",
    "\n",
    "1. **One to One RNNs (Vanilla Neural Networks):** One-to-one Networks are not actually RNNs but Vanilla Neural Networks which has a single entrance and single exit. These type of RNNs are mainly used for general machine learning problems.\n",
    "2. **One to Many RNNs:** An RNN variant where it takes a single input and generates a sequence of outputs. `For example:` Image Caption Generator\n",
    "3. **Many to One RNNs:** An RNN variant where it takes a sequence of inputs and produces a single output. `For example:` Sentiment Analysis\n",
    "4. **Many to Many RNNs  (Sequence-to-Sequence):** This variant takes a sequence of inputs and produces a sequence of outputs, matching the input sequence's length. `For example:` Machine Translation.\n",
    "5. **Seq2seq RNNs  (Sequence-to-Sequence):** In this architecture, there are two RNNs: an encoder RNN that processes the input sequence and summarizes it into a fixed-size context vector, and a decoder RNN that generates the output sequence based on this context vector. `For example:` Speech Recognition.\n",
    "\n",
    "![Types of RNN](./../../assets/rnn-types.jpg)\n",
    "\n",
    "## Application areas of RNNs\n",
    "\n",
    "- Natural Language Processing\n",
    "- Time Series Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time\n",
    "\n",
    "Backpropagation Through Time (BPTT) is a gradient-based optimization algorithm used to train recurrent neural networks (RNNs) and other sequential models. It is an extension of the standard backpropagation algorithm to handle sequences of data.\n",
    "\n",
    "![Back Propagation Through Time](./../../assets/bptt.jpg)\n",
    "\n",
    "To compute the gradients of the objective function with respect to all the decomposed model parameters, we can use the chain rule of calculus to propagate gradients through time. The process involves three main steps:\n",
    "\n",
    "### 1. Forward Pass\n",
    "\n",
    "During the forward pass, we compute the hidden states and output for each time step using the given equations:\n",
    "\n",
    "Hidden state at time step t:\n",
    "\n",
    "$h_t = activation(W_{hh}h_{tâˆ’1} + W_{xh}x_t)$\n",
    "\n",
    "Output at time step t:\n",
    "\n",
    "$o_t = W_{ho}h_t$\n",
    "\n",
    "Where:\n",
    "- $h_t$ is the hidden state at time step `t`.\n",
    "- $x_t$ is the input at time step `t`.\n",
    "- $W_{hh}$ is the weight matrix for the hidden-to-hidden connections.\n",
    "- $W_{xh}$ is the weight matrix for the input-to-hidden connections.\n",
    "- $W_{ho}$ is the weight matrix for the hidden-to-output connections.\n",
    "- `activation` is the identity mapping in this case.\n",
    "\n",
    "### 2. Loss Computation\n",
    "\n",
    "After obtaining the output $O_t$ for each time step, we compute the loss $L_t$ at each time stamp using a loss function (not specified in the given equations):\n",
    "\n",
    "$L_t = loss(O_t, y_t)$\n",
    "\n",
    "Where:\n",
    "- is the target at time step `t`.\n",
    "\n",
    "### 3. Backward Pass (BPTT)\n",
    "\n",
    "The goal of BPTT is to compute the gradients of the objective function with respect to all the model parameters (weights). Starting from the last time step (`T`), we backpropagate the gradients through time by iteratively applying the chain rule.\n",
    "\n",
    "- Initialize the gradient of the loss with respect to the output at the last time step:\n",
    "\n",
    "    $\\frac{\\partial L_T}{\\partial o_T} = \\text{gradient of loss}(o_T, y_T)$\n",
    "\n",
    "- For each time step t from T to 1:\n",
    "    - Compute the gradient of the loss with respect to the hidden state:\n",
    "\n",
    "        $\\frac{\\partial L_t}{\\partial h_t} = \\frac{\\partial L_t}{\\partial o_t} \\cdot \\frac{\\partial o_t}{\\partial h_t} + \\frac{\\partial L_{t+1}}{\\partial h_t} \\cdot \\frac{\\partial h_{t+1}}{\\partial h_t}$\n",
    "\n",
    "    - Compute the gradients of the loss with respect to the weights:\n",
    "\n",
    "        $\n",
    "        \\frac{\\partial L_t}{\\partial W_{ho}} = \\frac{\\partial L_t}{\\partial o_t} \\cdot \\frac{\\partial o_t}{\\partial W_{ho}} \\\\\n",
    "        \\frac{\\partial L_t}{\\partial W_{hh}} = \\frac{\\partial L_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{hh}} \\\\\n",
    "        \\frac{\\partial L_t}{\\partial W_{xh}} = \\frac{\\partial L_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{xh}}\n",
    "        $\n",
    "\n",
    "    - Update the gradient of the loss with respect to the output of the previous time step:\n",
    "\n",
    "        $\\frac{\\partial L_{t-1}}{\\partial o_{t-1}} = \\frac{\\partial L_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial o_{t-1}}$\n",
    "\n",
    "After computing the gradients for all time steps, we can update the model parameters using an optimization algorithm (e.g., stochastic gradient descent) to minimize the objective function.\n",
    "\n",
    "The computational graph, shown in the figure above, helps visualize the dependencies among model variables and parameters during the computation of the RNN. Each node in the graph represents a variable, and each edge represents a computation involving those variables. The graph helps in understanding how gradients flow through the network during backpropagation through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the RNN\n",
    "max_features = 10000  # Vocabulary size (use the top 10,000 most frequent words)\n",
    "maxlen = 500  # Maximum sequence length (truncate/pad sequences to this length)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad/truncate sequences to a fixed length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 38s 60ms/step - loss: 0.5402 - accuracy: 0.7088 - val_loss: 0.4029 - val_accuracy: 0.8212\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.3442 - accuracy: 0.8589 - val_loss: 0.3459 - val_accuracy: 0.8552\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 42s 66ms/step - loss: 0.2795 - accuracy: 0.8887 - val_loss: 0.3669 - val_accuracy: 0.8570\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.2420 - accuracy: 0.9060 - val_loss: 0.4136 - val_accuracy: 0.8582\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.2029 - accuracy: 0.9218 - val_loss: 0.3579 - val_accuracy: 0.8600\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.1665 - accuracy: 0.9377 - val_loss: 0.4419 - val_accuracy: 0.8288\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 43s 69ms/step - loss: 0.1317 - accuracy: 0.9525 - val_loss: 0.4572 - val_accuracy: 0.8562\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.0975 - accuracy: 0.9651 - val_loss: 0.6622 - val_accuracy: 0.8342\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 47s 76ms/step - loss: 0.0736 - accuracy: 0.9750 - val_loss: 0.6055 - val_accuracy: 0.8178\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0561 - accuracy: 0.9804 - val_loss: 0.6494 - val_accuracy: 0.8224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2da96a4d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 13ms/step - loss: 0.6633 - accuracy: 0.8186\n",
      "Test loss: 0.6633, Test accuracy: 0.8186\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program loads the IMDB dataset, which consists of movie reviews labeled with sentiment (positive or negative). It preprocesses the text data, creates a basic RNN model using TensorFlow's Sequential API, and trains the model on the training set. Finally, it evaluates the model on the test set and prints the test loss and accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
