{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Operations\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a class of deep learning models designed to process data with a grid-like structure, such as images, time series, or speech signals. CNNs have revolutionized the field of computer vision by achieving state-of-the-art performance in various tasks, including image classification, object detection, and image segmentation.\n",
    "\n",
    "At the core of CNNs are convolutional operations. Convolution is a mathematical operation that combines two functions to produce a third function. In the context of CNNs, the convolution operation is used to extract features from input data.\n",
    "\n",
    "## Types of Convolution Operations\n",
    "\n",
    "- **Continuous Convolution:** In continuous convolution, we have two continuous functions: the input function `x(t)` and the weighting function `w(t-a)`. The convolution operation is represented by the equation:\n",
    "\n",
    "    $s(t) = ∫ x(a)w(t - a)\\ da$\n",
    "\n",
    "    Here, `s(t)` represents the smoothed estimate of the input at time `t`. The integral is computed over the entire range of `a`.\n",
    "\n",
    "    `Example:` Suppose we have a laser sensor tracking the position of a spaceship. The laser sensor provides a noisy measurement of the spaceship's position at each moment. To obtain a less noisy estimate, we can apply a weighted average using a weighting function that gives more weight to recent measurements. In this case, `x(a)` represents the noisy measurements, and `w(t - a)` represents the weighting function. The integral is computed over the range of `a`. By convolving `x(a)` and `w(t - a)`, we obtain the smoothed estimate `s(t)` of the spaceship's position.\n",
    "\n",
    "- **Discrete Convolution:** In practical applications, data is often discrete and sampled at regular intervals. In discrete convolution, we have two discrete functions: the input function `x(t)` and the weighting function `w(t-a)`. The convolution operation is represented by the equation:\n",
    "\n",
    "    $s(t) = ∑ x(a)w(t - a)$\n",
    "\n",
    "    Here, the summation is taken over a finite range of values.\n",
    "\n",
    "    `Example:` Let's consider a discrete scenario where we have a sequence of temperature readings taken every hour. We want to apply a moving average filter to smooth out the temperature fluctuations. We can use a window of size 3 with equal weights. Here, `x(a)` represents the temperature readings, and `w(t - a)` represents the weights of the moving average filter. The summation is taken over the range of `a`. By convolving `x(a)` and `w(t - a)`, we obtain the smoothed sequence `s(t)` of temperature values.\n",
    "\n",
    "## Convolution in Multiple Dimensions\n",
    "\n",
    "Convolution can be extended to multiple dimensions, such as two-dimensional images. For example, in the case of a 2D image `I` and a 2D kernel `K`, the convolution operation is represented by the equation:\n",
    "\n",
    "$S(i, j) = ∑∑ I(m, n)\\ K(i - m, j - n)$\n",
    "\n",
    "Here, the double summation is taken over the spatial dimensions `(m, n)` of the input image. `S(i, j)` represents the filtered response of the input image to the kernel at position `(i, j)`.\n",
    "\n",
    "`Example:` Consider an image processing task where we have a grayscale image I and a 3x3 kernel K for edge detection. We want to convolve the image with the kernel to extract edge features. Here, `I(m, n)` represents the intensity values of the image at spatial coordinates `(m, n)`, and `K(i - m, j - n)` represents the values of the kernel. The double summation is taken over the spatial dimensions `(m, n)`. The resulting feature map `S(i, j)` represents the response of the image to the kernel at each location `(i, j)`, highlighting the edges.\n",
    "\n",
    "## Commutative Property and Cross-Correlation\n",
    "Convolution is a commutative operation, meaning that flipping the kernel relative to the input results in the same output. However, many machine learning libraries implement a related function called cross-correlation, which is equivalent to convolution without flipping the kernel. The cross-correlation operation is represented by the equation:\n",
    "\n",
    "$S(i, j) = ∑∑ I(i + m, j + n)\\ K(m, n)$\n",
    "\n",
    "Here, the spatial dimensions (m, n) of the kernel are not flipped.\n",
    "\n",
    "`Example:` Let's consider the same image processing scenario as before, but this time using cross-correlation instead of convolution. Here, the main difference is that the kernel `K(m, n)` is not flipped relative to the image `I`. The resulting feature map `S(i, j)` represents the response of the image to the kernel at each location `(i, j)`, without flipping the kernel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation as Matrix Multiplication\n",
    "\n",
    "Discrete convolution can be represented as matrix multiplication, where the input and kernel are reshaped into matrices. Specifically, in the case of two-dimensional convolution, a doubly block circulant matrix can be used to perform the convolution operation efficiently.\n",
    "\n",
    "A doubly block circulant matrix is a structured matrix that exhibits repetitive patterns and has entries constrained to be equal to other entries. It consists of circulant blocks arranged in a grid-like structure. Each circulant block corresponds to the convolution of a single element of the kernel with overlapping regions of the input.\n",
    "\n",
    "To illustrate this, let's consider a 3x3 input matrix I and a 2x2 kernel matrix K. The resulting feature map will have dimensions 2x2. We can reshape the input matrix I and the kernel matrix K into column vectors:\n",
    "$\n",
    "I = [I(1,1), I(1,2), I(1,3), I(2,1), I(2,2), I(2,3), I(3,1), I(3,2), I(3,3)]^T \\\\\n",
    "K = [K(1,1), K(1,2), K(2,1), K(2,2)]^T\n",
    "$\n",
    "\n",
    "Now, we construct a doubly block circulant matrix C using K as the circulant block:\n",
    "\n",
    "$\n",
    "C = [K(1,1), K(2,1), 0, K(1,2), K(2,2), 0, 0, 0, 0; \\\\\n",
    "K(2,1), K(1,1), K(1,2), K(2,2), K(1,1), K(1,2), 0, 0, 0; \\\\\n",
    "0, K(2,1), K(2,2), 0, K(1,2), K(2,2), 0, 0, 0; \\\\\n",
    "K(1,2), K(2,2), 0, K(1,1), K(2,1), 0, K(1,2), K(2,2), 0; \\\\\n",
    "K(2,2), K(1,2), K(2,1), K(1,2), K(2,2), 0, K(1,1), K(2,1), K(1,2); \\\\\n",
    "0, K(2,2), K(1,2), 0, K(2,1), K(1,2), 0, K(2,2), 0; \\\\\n",
    "0, 0, 0, K(1,2), K(2,2), 0, K(1,1), K(2,1), 0; \\\\\n",
    "0, 0, 0, K(2,2), K(1,2), K(2,1), K(2,1), K(1,1), K(1,2); \\\\\n",
    "0, 0, 0, 0, 0, 0, K(1,2), K(2,2), 0]\n",
    "$\n",
    "\n",
    "By multiplying the matrix C with the column vector I, we obtain the result as:\n",
    "\n",
    "$S = C * I$\n",
    "\n",
    "The resulting column vector S represents the flattened feature map. We can reshape it back into a 2x2 matrix to obtain the final feature map.\n",
    "\n",
    "The use of the doubly block circulant matrix allows us to perform convolution efficiently using matrix multiplication. The repetitive structure of the matrix reduces the computational complexity compared to traditional element-wise convolution.\n",
    "\n",
    "It's important to note that while the implementation of convolution as matrix multiplication is computationally efficient, it relies on specific properties of the convolution operation and may not be necessary from a theoretical perspective. Neural network libraries often optimize the convolution operation using various techniques, including specialized convolution algorithms and GPU acceleration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
