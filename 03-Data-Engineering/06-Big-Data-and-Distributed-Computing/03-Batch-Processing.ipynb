{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cf5a1b",
   "metadata": {},
   "source": [
    "# Batch Processing\n",
    "\n",
    "Batch processing is a data processing method in which a group of data is collected, processed, and analyzed together. In batch processing, data is collected and processed over a period of time, and then processed as a group rather than individually in real-time.\n",
    "\n",
    "Batch processing is useful when you have large volumes of data that can't be processed in real-time. Batch processing allows you to process data in parallel and optimize resource utilization. This can be done by dividing the data into smaller chunks, processing them separately, and then merging the results.\n",
    "\n",
    "Batch processing is often used in data warehousing, data analysis, and data mining applications. It is commonly used for processing large datasets, such as log files, customer data, and sales data.\n",
    "\n",
    "Batch processing can be implemented using a variety of tools and technologies. Some popular batch processing frameworks include Apache Hadoop, Apache Spark, and Apache Flink. These frameworks provide a distributed computing environment that allows you to process large volumes of data in parallel.\n",
    "\n",
    "In Python, you can use the PySpark API to perform batch processing on large datasets. PySpark is a Python library that provides a Python API for Apache Spark. You can use PySpark to read data from files, databases, or other sources, process the data using distributed computing, and write the results to files, databases, or other destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae1aa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:49:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Initial Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "| name|age|gender|\n",
      "+-----+---+------+\n",
      "| John| 25|  Male|\n",
      "| Mary| 30|Female|\n",
      "|  Bob| 20|  Male|\n",
      "|Alice| 35|Female|\n",
      "+-----+---+------+\n",
      "\n",
      "Filtered Dataset (Males Only):\n",
      "+----+---+------+\n",
      "|name|age|gender|\n",
      "+----+---+------+\n",
      "|John| 25|  Male|\n",
      "| Bob| 20|  Male|\n",
      "+----+---+------+\n",
      "\n",
      "Average Age of Males: 22.50\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
    "\n",
    "# Create a custom dataset\n",
    "data = [(\"John\", 25, \"Male\"), (\"Mary\", 30, \"Female\"), (\"Bob\", 20, \"Male\"), (\"Alice\", 35, \"Female\")]\n",
    "columns = [\"name\", \"age\", \"gender\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the dataset\n",
    "print(\"Initial Dataset:\")\n",
    "df.show()\n",
    "\n",
    "# Filter the dataset to only include males\n",
    "male_df = df.filter(df.gender == \"Male\")\n",
    "\n",
    "# Show the filtered dataset\n",
    "print(\"Filtered Dataset (Males Only):\")\n",
    "male_df.show()\n",
    "\n",
    "# Calculate the average age of males\n",
    "average_age = male_df.selectExpr(\"avg(age)\").collect()[0][0]\n",
    "\n",
    "# Print the average age of males\n",
    "print(\"Average Age of Males: {:.2f}\".format(average_age))\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a6af5",
   "metadata": {},
   "source": [
    "In this example, we first create a SparkSession using the `SparkSession.builder` method. Then, we create a custom dataset with the columns `\"name\", \"age\", and \"gender\"`. We show the initial dataset using the `show()` method.\n",
    "\n",
    "Next, we filter the dataset to only include males using the `filter()` method. We show the filtered dataset using the `show()` method.\n",
    "\n",
    "Then, we calculate the average age of males using the `selectExpr()` method to select the `\"age\"` column and calculate the average using the `avg()` function. We collect the result using the `collect()` method and extract the average age using indexing.\n",
    "\n",
    "Finally, we print the average age of males using the `format()` method and stop the SparkSession using the `stop()` method.\n",
    "\n",
    "Note that this is a very simple example and batch processing can involve much more complex operations depending on the dataset and use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
