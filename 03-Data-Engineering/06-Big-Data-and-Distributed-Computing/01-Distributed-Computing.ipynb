{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eef1858",
   "metadata": {},
   "source": [
    "# Distributed Computing\n",
    "\n",
    "Distributed computing is a method of computing that involves the use of multiple computers or nodes working together as a single system to perform a task or solve a problem. It is often used for tasks that are too large or complex to be performed by a single computer or for tasks that require high levels of computing power, storage, or bandwidth.\n",
    "\n",
    "In a distributed computing system, tasks are divided into smaller subtasks that are distributed among the nodes, and each node works on its assigned subtask in parallel with the other nodes. The nodes communicate with each other to coordinate their work and share data, and the results of the subtasks are combined to produce the final output.\n",
    "\n",
    "There are several benefits of distributed computing, including:\n",
    "\n",
    "- **Increased processing power:** By using multiple nodes to work on a task in parallel, distributed computing can greatly increase processing power and reduce the time needed to complete a task.\n",
    "- **Improved scalability:** Distributed computing systems can be scaled up or down easily by adding or removing nodes, making them ideal for tasks that require varying levels of computing power.\n",
    "- **Fault tolerance:** Distributed computing systems are often designed to be fault-tolerant, meaning that if one node fails, the other nodes can continue working on the task without interruption.\n",
    "- **Lower costs:** By using multiple low-cost computers instead of a single high-cost computer, distributed computing can be a more cost-effective way to perform certain tasks.\n",
    "\n",
    "Distributed computing is used in many different fields, including scientific computing, data processing, and web applications. Examples of distributed computing systems include Apache Hadoop, Apache Spark, and Amazon Web Services (AWS) Elastic MapReduce.\n",
    "\n",
    "Here's an example of using PySpark to process a large dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece7c402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:44:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.4820883370199694\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Generate a random dataset of 10000 samples with 10 features\n",
    "X = np.random.rand(10000, 10)\n",
    "y = np.random.randint(2, size=10000)\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Convert the NumPy arrays to Spark dataframes\n",
    "X_df = spark.createDataFrame(X.tolist())\n",
    "\n",
    "# Create a label column for y\n",
    "y_df = spark.createDataFrame([(int(label),) for label in y], [\"label\"])\n",
    "y_df = y_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Add an ID column to X_df\n",
    "X_df = X_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Combine the feature and label dataframes\n",
    "data = X_df.join(y_df, on=\"id\")\n",
    "\n",
    "# Combine the feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=X_df.columns[:-1], outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(training_data, testing_data) = data.randomSplit([0.7, 0.3], seed=12345)\n",
    "\n",
    "# Train a logistic regression model on the training data\n",
    "lr = LogisticRegression(maxIter=10, labelCol=\"label\")\n",
    "model = lr.fit(training_data)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = model.transform(testing_data)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
