{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4318432",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant and informative features from a larger set of available features for use in machine learning algorithms. The aim is to reduce the dimensionality of the data and improve the accuracy and efficiency of the model.\n",
    "\n",
    "There are several techniques of feature selection. Let's take a look into a two most popular techniques.\n",
    "\n",
    "## Forward Feature Selection\n",
    "\n",
    "Forward feature selection involves starting with an empty set of features and iteratively adding one feature at a time based on their individual performance in predicting the outcome variable. This process continues until a stopping criterion is met, such as reaching a predefined number of features or a specific level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5bde86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (Forward): [9] Score: 0.68125\n",
      "Selected Features (Forward): [9, 0] Score: 0.7\n",
      "Selected Features (Forward): [9, 0, 5] Score: 0.78125\n",
      "Selected Features (Forward): [9, 0, 5, 6] Score: 0.84375\n",
      "Selected Features (Forward): [9, 0, 5, 6, 2] Score: 0.85\n",
      "Selected Features (Forward): [9, 0, 5, 6, 2, 1] Score: 0.8375\n",
      "Selected Features (Forward): [9, 0, 5, 6, 2, 1, 7] Score: 0.8375\n",
      "Selected Features (Forward): [9, 0, 5, 6, 2, 1, 7, 8] Score: 0.8625\n",
      "Selected Features (Forward): [9, 0, 5, 6, 2, 1, 7, 8, 4] Score: 0.8625\n",
      "Selected Features (Forward): [9, 0, 5, 6, 2, 1, 7, 8, 4, 3] Score: 0.84375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create custom dataset\n",
    "X, y = make_classification(n_samples=800, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implement forward feature selection\n",
    "selected_features = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    best_accuracy = 0\n",
    "    best_feature = None\n",
    "    for j in range(X_train.shape[1]):\n",
    "        if j not in selected_features:\n",
    "            features = selected_features + [j]\n",
    "            model = LogisticRegression()\n",
    "            model.fit(X_train[:, features], y_train)\n",
    "            accuracy = model.score(X_test[:, features], y_test)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_feature = j\n",
    "    selected_features.append(best_feature)\n",
    "    print(\"Selected Features (Forward):\", selected_features, \"Score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1c2c9",
   "metadata": {},
   "source": [
    "This code creates a custom dataset, splits it into training and testing sets, and then implements forward feature selection. Note that this code assumes that you are using a binary classification problem and logistic regression as the classifier, but you can modify it to use other classifiers and handle different types of problems.\n",
    "\n",
    "## Backward Feature Selection\n",
    "\n",
    "Backward feature selection, on the other hand, starts with all available features and iteratively removes one feature at a time based on their individual performance in predicting the outcome variable. This process continues until a stopping criterion is met, such as reaching a predefined number of features or a specific level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8702aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (Backward): [0, 1, 2, 3, 4, 5, 6, 7, 8] Score: 0.7125\n",
      "Selected Features (Backward): [0, 1, 2, 3, 4, 5, 6, 7] Score: 0.65\n",
      "Selected Features (Backward): [0, 1, 2, 3, 4, 6, 7] Score: 0.6375\n",
      "Selected Features (Backward): [1, 2, 3, 4, 6, 7] Score: 0.59375\n",
      "Selected Features (Backward): [1, 2, 3, 4, 6] Score: 0.45625\n",
      "Selected Features (Backward): [2, 3, 4, 6] Score: 0.4625\n",
      "Selected Features (Backward): [2, 3, 4] Score: 0.4625\n",
      "Selected Features (Backward): [3, 4] Score: 0.5\n",
      "Selected Features (Backward): [4] Score: 0.50625\n"
     ]
    }
   ],
   "source": [
    "# Implement backward feature elimination\n",
    "selected_features = list(range(X_train.shape[1]))\n",
    "for i in range(X_train.shape[1] - 1):\n",
    "    worst_accuracy = 1\n",
    "    worst_feature = None\n",
    "    for j in selected_features:\n",
    "        features = selected_features.copy()\n",
    "        features.remove(j)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train[:, features], y_train)\n",
    "        accuracy = model.score(X_test[:, features], y_test)\n",
    "        if accuracy < worst_accuracy:\n",
    "            worst_accuracy = accuracy\n",
    "            worst_feature = j\n",
    "    selected_features.remove(worst_feature)\n",
    "    print(\"Selected Features (Backward):\", selected_features, \"Score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b23ca",
   "metadata": {},
   "source": [
    "This code creates a custom dataset, splits it into training and testing sets, and then implements backward feature elimination. Note that this code assumes that you are using a binary classification problem and logistic regression as the classifier, but you can modify it to use other classifiers and handle different types of problems.\n",
    "\n",
    "Both forward and backward feature selection have their own advantages and limitations. Forward feature selection tends to be more computationally efficient and is more likely to identify relevant features that may be missed in backward selection. However, it may also include irrelevant features that may not contribute to the overall accuracy of the model.\n",
    "\n",
    "In contrast, backward feature selection tends to produce more parsimonious models that may be easier to interpret and have better generalizability. However, it may also remove important features that may have a significant impact on the model's accuracy.\n",
    "\n",
    "Ultimately, the choice between forward and backward feature selection depends on the specific needs and characteristics of the dataset and the goals of the analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
